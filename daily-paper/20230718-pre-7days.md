# 2023-07-07 to 2023-07-18 Paper List 

## Paper 1 : Robotic Manipulation Datasets for Offline Compositional Reinforcement Learning
- _**Keywords**_: manipulation, robot, RL
- _**Abstract**_: Offline reinforcement learning (RL) is a promising direction that allows RL agents to pre-train on large datasets, avoiding the recurrence of expensive data collection. To advance the field, it is crucial to generate large-scale datasets. Compositional RL is particularly appealing for generating such large datasets, since 1) it permits creating many tasks from few components, 2) the task structure may enable trained agents to solve new tasks by combining relevant learned components, and 3) the compositional dimensions provide a notion of task relatedness. This paper provides four offline RL datasets for simulated robotic manipulation created using the 256 tasks from CompoSuite [Mendez et al., 2022a]. Each dataset is collected from an agent with a different degree of performance, and consists of 256 million transitions. We provide training and evaluation settings for assessing an agent's ability to learn compositional task policies. Our benchmarking experiments on each setting show that current offline RL methods can learn the training tasks to some extent and that compositional methods significantly outperform non-compositional methods. However, current methods are still unable to extract the tasks' compositional structure to generalize to unseen tasks, showing a need for further research in offline compositional RL. 
Robotic Manipulation Datasets for Offline Compositional Reinforcement Learning
 - _**标题**_: 用于离线组合强化学习的机器人操作数据集
- _**摘要**_: 离线强化学习（RL）是一个很有前途的方向，它允许 RL 代理在大型数据集上进行预训练，避免重复进行昂贵的数据收集。为了推进该领域的发展，生成大规模数据集至关重要。组合强化学习对于生成如此大的数据集特别有吸引力，因为 1）它允许从很少的组件创建许多任务，2）任务结构可以使训练有素的代理能够通过组合相关的学习组件来解决新任务，3）组合维度提供了任务相关性的概念。本文提供了四个离线 RL 数据集，用于使用 CompoSuite 中的 256 个任务创建的模拟机器人操作 [Mendez et al., 2022a]。每个数据集都是从具有不同性能程度的代理收集的，由 2.56 亿个转换组成。我们提供培训和评估设置，用于评估代理学习组合任务策略的能力。我们对每个设置的基准测试表明，当前的离线强化学习方法可以在一定程度上学习训练任务，并且组合方法显着优于非组合方法。然而，当前的方法仍然无法提取任务的组合结构以推广到未见过的任务，这表明需要对离线组合强化学习进行进一步的研究。 
- _**url**_: http://arxiv.org/abs/2307.07091v1


## Paper 2 : Meta-Policy Learning over Plan Ensembles for Robust Articulated Object Manipulation
- _**Keywords**_: manipulation, robot, RL
- _**Abstract**_: Recent work has shown that complex manipulation skills, such as pushing or pouring, can be learned through state-of-the-art learning based techniques, such as Reinforcement Learning (RL). However, these methods often have high sample-complexity, are susceptible to domain changes, and produce unsafe motions that a robot should not perform. On the other hand, purely geometric model-based planning can produce complex behaviors that satisfy all the geometric constraints of the robot but might not be dynamically feasible for a given environment. In this work, we leverage a geometric model-based planner to build a mixture of path-policies on which a task-specific meta-policy can be learned to complete the task. In our results, we demonstrate that a successful meta-policy can be learned to push a door, while requiring little data and being robust to model uncertainty of the environment. We tested our method on a 7-DOF Franka-Emika Robot pushing a cabinet door in simulation. 
Meta-Policy Learning over Plan Ensembles for Robust Articulated Object Manipulation
 - _**标题**_: 用于鲁棒铰接对象操作的计划集成的元策略学习
- _**摘要**_: 最近的研究表明，可以通过最先进的基于学习的技术（例如强化学习（RL））来学习复杂的操作技能，例如推动或倾倒。然而，这些方法通常具有很高的样本复杂性，容易受到域变化的影响，并且会产生机器人不应执行的不安全运动。另一方面，纯粹基于几何模型的规划可以产生满足机器人所有几何约束的复杂行为，但对于给定环境可能不是动态可行的。在这项工作中，我们利用基于几何模型的规划器来构建路径策略的混合，可以在其中学习特定于任务的元策略来完成任务。在我们的结果中，我们证明了可以学习成功的元策略来推开一扇门，同时需要很少的数据并且对环境的不确定性建模具有鲁棒性。我们在 7 自由度 Franka-Emika 机器人上模拟推动柜门测试了我们的方法。 
- _**url**_: http://arxiv.org/abs/2307.04040v1


## Paper 3 : Magnetic Field-Based Reward Shaping for Goal-Conditioned Reinforcement Learning
- _**Keywords**_: manipulation, robot, RL
- _**Abstract**_: Goal-conditioned reinforcement learning (RL) is an interesting extension of the traditional RL framework, where the dynamic environment and reward sparsity can cause conventional learning algorithms to fail. Reward shaping is a practical approach to improving sample efficiency by embedding human domain knowledge into the learning process. Existing reward shaping methods for goal-conditioned RL are typically built on distance metrics with a linear and isotropic distribution, which may fail to provide sufficient information about the ever-changing environment with high complexity. This paper proposes a novel magnetic field-based reward shaping (MFRS) method for goal-conditioned RL tasks with dynamic target and obstacles. Inspired by the physical properties of magnets, we consider the target and obstacles as permanent magnets and establish the reward function according to the intensity values of the magnetic field generated by these magnets. The nonlinear and anisotropic distribution of the magnetic field intensity can provide more accessible and conducive information about the optimization landscape, thus introducing a more sophisticated magnetic reward compared to the distance-based setting. Further, we transform our magnetic reward to the form of potential-based reward shaping by learning a secondary potential function concurrently to ensure the optimal policy invariance of our method. Experiments results in both simulated and real-world robotic manipulation tasks demonstrate that MFRS outperforms relevant existing methods and effectively improves the sample efficiency of RL algorithms in goal-conditioned tasks with various dynamics of the target and obstacles. 
Magnetic Field-Based Reward Shaping for Goal-Conditioned Reinforcement Learning
 - _**标题**_: 用于目标条件强化学习的基于磁场的奖励塑造
- _**摘要**_: 目标条件强化学习 (RL) 是传统 RL 框架的有趣扩展，其中动态环境和奖励稀疏性可能会导致传统学习算法失败。奖励塑造是一种通过将人类领域知识嵌入学习过程来提高样本效率的实用方法。目标条件强化学习的现有奖励塑造方法通常建立在具有线性和各向同性分布的距离度量上，这可能无法提供有关不断变化的高复杂性环境的足够信息。本文提出了一种新颖的基于磁场的奖励塑造（MFRS）方法，用于具有动态目标和障碍物的目标条件强化学习任务。受磁铁物理特性的启发，我们将目标和障碍物视为永磁铁，并根据这些磁铁产生的磁场强度值建立奖励函数。磁场强度的非线性和各向异性分布可以提供有关优化景观的更容易访问和有利的信息，从而与基于距离的设置相比引入更复杂的磁奖励。此外，我们通过同时学习次级势函数，将磁性奖励转化为基于势的奖励塑造的形式，以确保我们的方法的最佳策略不变性。模拟和现实机器人操作任务中的实验结果表明，MFRS 优于现有的相关方法，并有效提高了 RL 算法在具有各种目标和障碍物动态的目标条件任务中的样本效率。 
- _**url**_: http://arxiv.org/abs/2307.08033v1


## Paper 4 : Boosting Feedback Efficiency of Interactive Reinforcement Learning by Adaptive Learning from Scores
- _**Keywords**_: manipulation, robot, RL
- _**Abstract**_: Interactive reinforcement learning has shown promise in learning complex robotic tasks. However, the process can be human-intensive due to the requirement of large amount of interactive feedback. This paper presents a new method that uses scores provided by humans, instead of pairwise preferences, to improve the feedback efficiency of interactive reinforcement learning. Our key insight is that scores can yield significantly more data than pairwise preferences. Specifically, we require a teacher to interactively score the full trajectories of an agent to train a behavioral policy in a sparse reward environment. To avoid unstable scores given by human negatively impact the training process, we propose an adaptive learning scheme. This enables the learning paradigm to be insensitive to imperfect or unreliable scores. We extensively evaluate our method on robotic locomotion and manipulation tasks. The results show that the proposed method can efficiently learn near-optimal policies by adaptive learning from scores, while requiring less feedback compared to pairwise preference learning methods. The source codes are publicly available at https://github.com/SSKKai/Interactive-Scoring-IRL. 
Boosting Feedback Efficiency of Interactive Reinforcement Learning by Adaptive Learning from Scores
 - _**标题**_: 通过分数自适应学习提高交互式强化学习的反馈效率
- _**摘要**_: 交互式强化学习在学习复杂的机器人任务方面显示出了前景。然而，由于需要大量的交互式反馈，该过程可能是人力密集型的。本文提出了一种新方法，使用人类提供的分数而不是成对偏好来提高交互式强化学习的反馈效率。我们的主要见解是，分数可以比成对偏好产生更多的数据。具体来说，我们要求教师以交互方式对智能体的完整轨迹进行评分，以在稀疏奖励环境中训练行为策略。为了避免人类给出的不稳定分数对训练过程产生负面影响，我们提出了一种自适应学习方案。这使得学习范式对不完美或不可靠的分数不敏感。我们广泛评估了我们在机器人运动和操纵任务方面的方法。结果表明，所提出的方法可以通过分数的自适应学习来有效地学习接近最优的策略，同时与成对偏好学习方法相比需要更少的反馈。源代码可在 https://github.com/SSKKai/Interactive-Scoring-IRL 上公开获取。 
- _**url**_: http://arxiv.org/abs/2307.05405v1


## Paper 5 : Seeing is not Believing: Robust Reinforcement Learning against Spurious Correlation
- _**Keywords**_: manipulation, RL
- _**Abstract**_: Robustness has been extensively studied in reinforcement learning (RL) to handle various forms of uncertainty such as random perturbations, rare events, and malicious attacks. In this work, we consider one critical type of robustness against spurious correlation, where different portions of the state do not have causality but have correlations induced by unobserved confounders. These spurious correlations are ubiquitous in real-world tasks, for instance, a self-driving car usually observes heavy traffic in the daytime and light traffic at night due to unobservable human activity. A model that learns such useless or even harmful correlation could catastrophically fail when the confounder in the test case deviates from the training one. Although motivated, enabling robustness against spurious correlation poses significant challenges since the uncertainty set, shaped by the unobserved confounder and sequential structure of RL, is difficult to characterize and identify. Existing robust algorithms that assume simple and unstructured uncertainty sets are therefore inadequate to address this challenge. To solve this issue, we propose Robust State-Confounded Markov Decision Processes (RSC-MDPs) and theoretically demonstrate its superiority in breaking spurious correlations compared with other robust RL counterparts. We also design an empirical algorithm to learn the robust optimal policy for RSC-MDPs, which outperforms all baselines in eight realistic self-driving and manipulation tasks. 
Seeing is not Believing: Robust Reinforcement Learning against Spurious Correlation
 - _**标题**_: 眼见为实：针对虚假相关性的鲁棒强化学习
- _**摘要**_: 鲁棒性在强化学习（RL）中得到了广泛的研究，以处理各种形式的不确定性，例如随机扰动、罕见事件和恶意攻击。在这项工作中，我们考虑了一种针对虚假相关性的关键鲁棒性，其中状态的不同部分没有因果关系，但具有由未观察到的混杂因素引起的相关性。这些虚假的相关性在现实世界的任务中普遍存在，例如，由于无法观察到的人类活动，自动驾驶汽车通常会在白天观察到交通拥堵，而在夜间观察到交通畅通。当测试用例中的混杂因素偏离训练混杂因素时，学习这种无用甚至有害相关性的模型可能会灾难性地失败。尽管有动机，但实现对抗虚假相关性的鲁棒性提出了重大挑战，因为由未观察到的混杂因素和强化学习的顺序结构形成的不确定性集很难表征和识别。因此，假设简单且非结构化不确定性集的现有稳健算法不足以应对这一挑战。为了解决这个问题，我们提出了鲁棒状态混杂马尔可夫决策过程（RSC-MDP），并从理论上证明了与其他鲁棒强化学习对应物相比，它在打破虚假相关性方面的优越性。我们还设计了一种经验算法来学习 RSC-MDP 的稳健最优策略，该策略在八个现实的自动驾驶和操纵任务中优于所有基线。 
- _**url**_: http://arxiv.org/abs/2307.07907v1


## Paper 6 : VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models
- _**Keywords**_: robot, LLM
- _**Abstract**_: Large language models (LLMs) are shown to possess a wealth of actionable knowledge that can be extracted for robot manipulation in the form of reasoning and planning. Despite the progress, most still rely on pre-defined motion primitives to carry out the physical interactions with the environment, which remains a major bottleneck. In this work, we aim to synthesize robot trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a large variety of manipulation tasks given an open-set of instructions and an open-set of objects. We achieve this by first observing that LLMs excel at inferring affordances and constraints given a free-form language instruction. More importantly, by leveraging their code-writing capabilities, they can interact with a visual-language model (VLM) to compose 3D value maps to ground the knowledge into the observation space of the agent. The composed value maps are then used in a model-based planning framework to zero-shot synthesize closed-loop robot trajectories with robustness to dynamic perturbations. We further demonstrate how the proposed framework can benefit from online experiences by efficiently learning a dynamics model for scenes that involve contact-rich interactions. We present a large-scale study of the proposed method in both simulated and real-robot environments, showcasing the ability to perform a large variety of everyday manipulation tasks specified in free-form natural language. Project website: https://voxposer.github.io 
VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models
 - _**标题**_: VoxPoser：使用语言模型进行机器人操作的可组合 3D 值图
- _**摘要**_: 大型语言模型（LLM）被证明拥有丰富的可操作知识，可以以推理和规划的形式提取这些知识用于机器人操作。尽管取得了进展，但大多数仍然依赖预定义的运动基元来执行与环境的物理交互，这仍然是一个主要瓶颈。在这项工作中，我们的目标是合成机器人轨迹，即 6-DoF 末端执行器路径点的密集序列，用于在给定开放指令集和开放对象集的情况下执行各种操作任务。我们首先观察到法学硕士擅长在自由形式的语言教学中推断可供性和约束，从而实现这一目标。更重要的是，通过利用其代码编写能力，他们可以与视觉语言模型 (VLM) 交互以组成 3D 值图，将知识融入代理的观察空间中。然后，将组合的值图用于基于模型的规划框架，以零样本合成具有动态扰动鲁棒性的闭环机器人轨迹。我们进一步展示了所提出的框架如何通过有效地学习涉及丰富接触交互的场景的动态模型来从在线体验中受益。我们在模拟和真实机器人环境中对所提出的方法进行了大规模研究，展示了执行以自由形式自然语言指定的各种日常操作任务的能力。项目网站：https://voxposer.github.io 
- _**url**_: http://arxiv.org/abs/2307.05973v1


## Paper 7 : SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Task Planning
- _**Keywords**_: robot, LLM
- _**Abstract**_: Large language models (LLMs) have demonstrated impressive results in developing generalist planning agents for diverse tasks. However, grounding these plans in expansive, multi-floor, and multi-room environments presents a significant challenge for robotics. We introduce SayPlan, a scalable approach to LLM-based, large-scale task planning for robotics using 3D scene graph (3DSG) representations. To ensure the scalability of our approach, we: (1) exploit the hierarchical nature of 3DSGs to allow LLMs to conduct a semantic search for task-relevant subgraphs from a smaller, collapsed representation of the full graph; (2) reduce the planning horizon for the LLM by integrating a classical path planner and (3) introduce an iterative replanning pipeline that refines the initial plan using feedback from a scene graph simulator, correcting infeasible actions and avoiding planning failures. We evaluate our approach on two large-scale environments spanning up to 3 floors, 36 rooms and 140 objects, and show that our approach is capable of grounding large-scale, long-horizon task plans from abstract, and natural language instruction for a mobile manipulator robot to execute. 
SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Task Planning
 - _**标题**_: SayPlan：使用 3D 场景图为大型语言模型奠定基础，实现可扩展的任务规划
- _**摘要**_: 大型语言模型（LLM）在开发用于不同任务的通用规划代理方面取得了令人印象深刻的成果。然而，在广阔的多层、多房间环境中实施这些计划对机器人技术提出了重大挑战。我们介绍 SayPlan，这是一种使用 3D 场景图 (3DSG) 表示的基于 LLM 的大规模机器人任务规划方法。为了确保我们方法的可扩展性，我们：（1）利用 3DSG 的层次性质，允许 LLM 从完整图的较小、折叠表示中对与任务相关的子图进行语义搜索； (2) 通过集成经典路径规划器来缩短法学硕士的规划范围，(3) 引入迭代重新规划流程，使用场景图模拟器的反馈来完善初始计划，纠正不可行的操作并避免规划失败。我们在两个跨越 3 层楼、36 个房间和 140 个物体的大型环境中评估了我们的方法，并表明我们的方法能够根据移动设备的抽象和自然语言指令来制定大规模、长期的任务计划。机械手机器人来执行。 
- _**url**_: http://arxiv.org/abs/2307.06135v1


## Paper 8 : RoCo: Dialectic Multi-Robot Collaboration with Large Language Models
- _**Keywords**_: robot, LLM
- _**Abstract**_: We propose a novel approach to multi-robot collaboration that harnesses the power of pre-trained large language models (LLMs) for both high-level communication and low-level path planning. Robots are equipped with LLMs to discuss and collectively reason task strategies. They then generate sub-task plans and task space waypoint paths, which are used by a multi-arm motion planner to accelerate trajectory planning. We also provide feedback from the environment, such as collision checking, and prompt the LLM agents to improve their plan and waypoints in-context. For evaluation, we introduce RoCoBench, a 6-task benchmark covering a wide range of multi-robot collaboration scenarios, accompanied by a text-only dataset for agent representation and reasoning. We experimentally demonstrate the effectiveness of our approach -- it achieves high success rates across all tasks in RoCoBench and adapts to variations in task semantics. Our dialog setup offers high interpretability and flexibility -- in real world experiments, we show RoCo easily incorporates human-in-the-loop, where a user can communicate and collaborate with a robot agent to complete tasks together. See project website https://project-roco.github.io for videos and code. 
RoCo: Dialectic Multi-Robot Collaboration with Large Language Models
 - _**标题**_: RoCo：具有大型语言模型的辩证多机器人协作
- _**摘要**_: 我们提出了一种新颖的多机器人协作方法，该方法利用预先训练的大语言模型（LLM）的力量来进行高级通信和低级路径规划。机器人配备了法学硕士来讨论和集体推理任务策略。然后，它们生成子任务计划和任务空间路径点路径，多臂运动规划器使用它们来加速轨迹规划。我们还提供来自环境的反馈，例如碰撞检查，并提示 LLM 代理改进他们的计划和路径点。为了进行评估，我们引入了 RoCoBench，这是一个涵盖广泛的多机器人协作场景的 6 任务基准，并附有用于代理表示和推理的纯文本数据集。我们通过实验证明了我们方法的有效性——它在 RoCoBench 中的所有任务中实现了很高的成功率，并适应任务语义的变化。我们的对话设置提供了高度的可解释性和灵活性——在现实世界的实验中，我们展示了 RoCo 可以轻松地融入人机交互，用户可以与机器人代理进行交流和协作，共同完成任务。有关视频和代码，请参阅项目网站 https://project-roco.github.io。 
- _**url**_: http://arxiv.org/abs/2307.04738v1


## Paper 9 : Large Language Models as General Pattern Machines
- _**Keywords**_: robot, LLM
- _**Abstract**_: We observe that pre-trained large language models (LLMs) are capable of autoregressively completing complex token sequences -- from arbitrary ones procedurally generated by probabilistic context-free grammars (PCFG), to more rich spatial patterns found in the Abstract Reasoning Corpus (ARC), a general AI benchmark, prompted in the style of ASCII art. Surprisingly, pattern completion proficiency can be partially retained even when the sequences are expressed using tokens randomly sampled from the vocabulary. These results suggest that without any additional training, LLMs can serve as general sequence modelers, driven by in-context learning. In this work, we investigate how these zero-shot capabilities may be applied to problems in robotics -- from extrapolating sequences of numbers that represent states over time to complete simple motions, to least-to-most prompting of reward-conditioned trajectories that can discover and represent closed-loop policies (e.g., a stabilizing controller for CartPole). While difficult to deploy today for real systems due to latency, context size limitations, and compute costs, the approach of using LLMs to drive low-level control may provide an exciting glimpse into how the patterns among words could be transferred to actions. 
Large Language Models as General Pattern Machines
 - _**标题**_: 作为通用模式机的大型语言模型
- _**摘要**_: 我们观察到，预训练的大语言模型（LLM）能够自回归完成复杂的标记序列——从概率上下文无关语法（PCFG）程序生成的任意标记序列，到抽象推理语料库（ARC）中发现的更丰富的空间模式），一个通用的人工智能基准，以 ASCII 艺术风格提示。令人惊讶的是，即使使用从词汇表中随机采样的标记来表达序列，也可以部分保留模式完成能力。这些结果表明，无需任何额外的培训，法学硕士就可以在上下文学习的驱动下充当通用序列建模者。在这项工作中，我们研究了如何将这些零样本能力应用于机器人技术中的问题——从推断代表随时间变化的状态的数字序列来完成简单的运动，到从最小到最大的奖励条件轨迹的提示，这些轨迹可以发现并表示闭环策略（例如 CartPole 的稳定控制器）。虽然由于延迟、上下文大小限制和计算成本，目前很难在实际系统中部署，但使用 LLM 驱动低级控制的方法可能会令人兴奋地了解单词之间的模式如何转换为操作。 
- _**url**_: http://arxiv.org/abs/2307.04721v1


## Paper 10 : Coupling Large Language Models with Logic Programming for Robust and General Reasoning from Text
- _**Keywords**_: robot, LLM
- _**Abstract**_: While large language models (LLMs), such as GPT-3, appear to be robust and general, their reasoning ability is not at a level to compete with the best models trained for specific natural language reasoning problems. In this study, we observe that a large language model can serve as a highly effective few-shot semantic parser. It can convert natural language sentences into a logical form that serves as input for answer set programs, a logic-based declarative knowledge representation formalism. The combination results in a robust and general system that can handle multiple question-answering tasks without requiring retraining for each new task. It only needs a few examples to guide the LLM's adaptation to a specific task, along with reusable ASP knowledge modules that can be applied to multiple tasks. We demonstrate that this method achieves state-of-the-art performance on several NLP benchmarks, including bAbI, StepGame, CLUTRR, and gSCAN. Additionally, it successfully tackles robot planning tasks that an LLM alone fails to solve. 
Coupling Large Language Models with Logic Programming for Robust and General Reasoning from Text
 - _**标题**_: 将大型语言模型与逻辑编程相结合，从文本中进行稳健且通用的推理
- _**摘要**_: 虽然大型语言模型 (LLM)，例如 GPT-3，看起来稳健且通用，但它们的推理能力还无法与针对特定自然语言推理问题训练的最佳模型竞争。在这项研究中，我们观察到大型语言模型可以充当高效的小样本语义解析器。它可以将自然语言句子转换为逻辑形式，作为答案集程序的输入，这是一种基于逻辑的声明性知识表示形式。这种组合产生了一个强大且通用的系统，可以处理多个问答任务，而无需为每个新任务进行重新训练。它只需要几个例子来指导LLM适应特定的任务，以及可重复使用的可应用于多个任务的ASP知识模块。我们证明该方法在多个 NLP 基准上实现了最先进的性能，包括 bAbI、StepGame、CLUTRR 和 gSCAN。此外，它还成功解决了仅法学硕士无法解决的机器人规划任务。 
- _**url**_: http://arxiv.org/abs/2307.07696v1


## Paper 11 : TransPose: A Transformer-based 6D Object Pose Estimation Network with Depth Refinement
- _**Keywords**_: manipulation, robot
- _**Abstract**_: As demand for robotics manipulation application increases, accurate vision-based 6D pose estimation becomes essential for autonomous operations. Convolutional Neural Networks (CNNs) based approaches for pose estimation have been previously introduced. However, the quest for better performance still persists especially for accurate robotics manipulation. This quest extends to the Agri-robotics domain. In this paper, we propose TransPose, an improved Transformer-based 6D pose estimation with a depth refinement module. The architecture takes in only an RGB image as input with no additional supplementing modalities such as depth or thermal images. The architecture encompasses an innovative lighter depth estimation network that estimates depth from an RGB image using feature pyramid with an up-sampling method. A transformer-based detection network with additional prediction heads is proposed to directly regress the object's centre and predict the 6D pose of the target. A novel depth refinement module is then used alongside the predicted centers, 6D poses and depth patches to refine the accuracy of the estimated 6D pose. We extensively compared our results with other state-of-the-art methods and analysed our results for fruit-picking applications. The results we achieved show that our proposed technique outperforms the other methods available in the literature. 
TransPose: A Transformer-based 6D Object Pose Estimation Network with Depth Refinement
 - _**标题**_: TransPose：基于 Transformer 的深度细化 6D 物体姿态估计网络
- _**摘要**_: 随着机器人操纵应用需求的增加，基于视觉的准确 6D 姿态估计对于自主操作变得至关重要。之前已经介绍过基于卷积神经网络（CNN）的姿态估计方法。然而，对更好性能的追求仍然存在，特别是对于精确的机器人操作。这一探索延伸到农业机器人领域。在本文中，我们提出了 TransPose，一种改进的基于 Transformer 的 6D 姿态估计，具有深度细化模块。该架构仅采用 RGB 图像作为输入，没有额外的补充模式，例如深度或热图像。该架构包含一个创新的较轻深度估计网络，该网络使用特征金字塔和上采样方法来估计 RGB 图像的深度。提出了一种带有附加预测头的基于变压器的检测网络，以直接回归对象的中心并预测目标的 6D 姿态。然后，将新颖的深度细化模块与预测中心、6D 姿势和深度补丁一起使用，以细化估计的 6D 姿势的准确性。我们将我们的结果与其他最先进的方法进行了广泛比较，并分析了我们在水果采摘应用中的结果。我们取得的结果表明，我们提出的技术优于文献中的其他方法。 
- _**url**_: http://arxiv.org/abs/2307.05561v1


## Paper 12 : Taming the Panda with Python: A Powerful Duo for Seamless Robotics Programming and Integration
- _**Keywords**_: manipulation, robot
- _**Abstract**_: Franka Emika robots have gained significant popularity in research and education due to their exceptional versatility and advanced capabilities. This work introduces panda-py - a Python interface and framework designed to empower Franka Emika robotics with accessible and efficient programming. The panda-py interface enhances the usability of Franka Emika robots, enabling researchers and educators to interact with them more effectively. By leveraging Python's simplicity and readability, users can quickly grasp the necessary programming concepts for robot control and manipulation. Moreover, integrating panda-py with other widely used Python packages in domains such as computer vision and machine learning amplifies the robot's capabilities. Researchers can seamlessly leverage the vast ecosystem of Python libraries, thereby enabling advanced perception, decision-making, and control functionalities. This compatibility facilitates the efficient development of sophisticated robotic applications, integrating state-of-the-art techniques from diverse domains without the added complexity of ROS. 
Taming the Panda with Python: A Powerful Duo for Seamless Robotics Programming and Integration
 - _**标题**_: 用 Python 驯服熊猫：无缝机器人编程和集成的强大组合
- _**摘要**_: Franka Emika 机器人因其卓越的多功能性和先进功能而在研究和教育领域广受欢迎。这项工作介绍了 panda-py——一个 Python 接口和框架，旨在为 Franka Emika 机器人技术提供可访问且高效的编程能力。 panda-py 界面增强了 Franka Emika 机器人的可用性，使研究人员和教育工作者能够更有效地与它们互动。通过利用Python的简单性和可读性，用户可以快速掌握机器人控制和操作所需的编程概念。此外，将 panda-py 与计算机视觉和机器学习等领域其他广泛使用的 Python 包集成可以增强机器人的功能。研究人员可以无缝地利用庞大的 Python 库生态系统，从而实现先进的感知、决策和控制功能。这种兼容性有助于高效开发复杂的机器人应用程序，集成来自不同领域的最先进技术，而不会增加 ROS 的复杂性。 
- _**url**_: http://arxiv.org/abs/2307.07633v1


## Paper 13 : Switching Head-Tail Funnel UNITER for Dual Referring Expression Comprehension with Fetch-and-Carry Tasks
- _**Keywords**_: manipulation, robot
- _**Abstract**_: This paper describes a domestic service robot (DSR) that fetches everyday objects and carries them to specified destinations according to free-form natural language instructions. Given an instruction such as "Move the bottle on the left side of the plate to the empty chair," the DSR is expected to identify the bottle and the chair from multiple candidates in the environment and carry the target object to the destination. Most of the existing multimodal language understanding methods are impractical in terms of computational complexity because they require inferences for all combinations of target object candidates and destination candidates. We propose Switching Head-Tail Funnel UNITER, which solves the task by predicting the target object and the destination individually using a single model. Our method is validated on a newly-built dataset consisting of object manipulation instructions and semi photo-realistic images captured in a standard Embodied AI simulator. The results show that our method outperforms the baseline method in terms of language comprehension accuracy. Furthermore, we conduct physical experiments in which a DSR delivers standardized everyday objects in a standardized domestic environment as requested by instructions with referring expressions. The experimental results show that the object grasping and placing actions are achieved with success rates of more than 90%. 
Switching Head-Tail Funnel UNITER for Dual Referring Expression Comprehension with Fetch-and-Carry Tasks
 - _**标题**_: 切换头尾漏斗 UNITER 以实现具有获取和携带任务的双重引用表达理解
- _**摘要**_: 本文描述了一种家庭服务机器人（DSR），它可以根据自由形式的自然语言指令获取日常物品并将其运送到指定目的地。给出诸如“将盘子左侧的瓶子移到空椅子上”之类的指令，DSR 有望从环境中的多个候选者中识别出瓶子和椅子，并将目标物体运送到目的地。大多数现有的多模态语言理解方法在计算复杂性方面是不切实际的，因为它们需要对目标对象候选和目标候选的所有组合进行推理。我们提出了 Switching Head-Tail Funnel UNITER，它通过使用单个模型单独预测目标对象和目的地来解决任务。我们的方法在一个新建的数据集上进行了验证，该数据集由对象操作指令和在标准 Embodied AI 模拟器中捕获的半真实感图像组成。结果表明，我们的方法在语言理解准确性方面优于基线方法。此外，我们还进行了物理实验，其中 DSR 按照带有引用表达式的指令的要求，在标准化的家庭环境中交付标准化的日常物品。实验结果表明，实现物体抓取和放置动作，成功率达到90%以上。 
- _**url**_: http://arxiv.org/abs/2307.07166v1


## Paper 14 : SAR: Generalization of Physiological Agility and Dexterity via Synergistic Action Representation
- _**Keywords**_: manipulation, robot
- _**Abstract**_: Learning effective continuous control policies in high-dimensional systems, including musculoskeletal agents, remains a significant challenge. Over the course of biological evolution, organisms have developed robust mechanisms for overcoming this complexity to learn highly sophisticated strategies for motor control. What accounts for this robust behavioral flexibility? Modular control via muscle synergies, i.e. coordinated muscle co-contractions, is considered to be one putative mechanism that enables organisms to learn muscle control in a simplified and generalizable action space. Drawing inspiration from this evolved motor control strategy, we use physiologically accurate human hand and leg models as a testbed for determining the extent to which a Synergistic Action Representation (SAR) acquired from simpler tasks facilitates learning more complex tasks. We find in both cases that SAR-exploiting policies significantly outperform end-to-end reinforcement learning. Policies trained with SAR were able to achieve robust locomotion on a wide set of terrains with high sample efficiency, while baseline approaches failed to learn meaningful behaviors. Additionally, policies trained with SAR on a multiobject manipulation task significantly outperformed (>70% success) baseline approaches (<20% success). Both of these SAR-exploiting policies were also found to generalize zero-shot to out-of-domain environmental conditions, while policies that did not adopt SAR failed to generalize. Finally, we establish the generality of SAR on broader high-dimensional control problems using a robotic manipulation task set and a full-body humanoid locomotion task. To the best of our knowledge, this investigation is the first of its kind to present an end-to-end pipeline for discovering synergies and using this representation to learn high-dimensional continuous control across a wide diversity of tasks. 
SAR: Generalization of Physiological Agility and Dexterity via Synergistic Action Representation
 - _**标题**_: SAR：通过协同动作表征概括生理敏捷性和灵巧性
- _**摘要**_: 在包括肌肉骨骼代理在内的高维系统中学习有效的连续控制策略仍然是一个重大挑战。在生物进化过程中，生物体已经发展出强大的机制来克服这种复杂性，以学习高度复杂的运动控制策略。是什么导致了这种强大的行为灵活性？通过肌肉协同作用的模块化控制，即协调的肌肉共同收缩，被认为是一种假定的机制，使生物体能够在简化且可概括的动作空间中学习肌肉控制。从这种进化的运动控制策略中汲取灵感，我们使用生理上准确的人类手和腿模型作为测试平台，以确定从更简单的任务中获得的协同动作表示（SAR）在多大程度上有助于学习更复杂的任务。我们发现在这两种情况下，SAR 利用策略的性能都显着优于端到端强化学习。使用 SAR 训练的策略能够以高样本效率在广泛的地形上实现稳健的运动，而基线方法无法学习有意义的行为。此外，在多对象操作任务上使用 SAR 训练的策略显着优于（>70% 成功）基线方法（<20% 成功）。研究还发现，这两种 SAR 利用策略都可以将零样本推广到域外环境条件，而未采用 SAR 的策略则无法推广。最后，我们使用机器人操纵任务集和全身人形运动任务建立了 SAR 在更广泛的高维控制问题上的通用性。据我们所知，这项研究是同类研究中第一个提出端到端管道，用于发现协同作用并使用这种表示来学习跨各种任务的高维连续控制。 
- _**url**_: http://arxiv.org/abs/2307.03716v2


## Paper 15 : Proximity and Visuotactile Point Cloud Fusion for Contact Patches in Extreme Deformation
- _**Keywords**_: manipulation, robot
- _**Abstract**_: Equipping robots with the sense of touch is critical to emulating the capabilities of humans in real world manipulation tasks. Visuotactile sensors are a popular tactile sensing strategy due to data output compatible with computer vision algorithms and accurate, high resolution estimates of local object geometry. However, these sensors struggle to accommodate high deformations of the sensing surface during object interactions, hindering more informative contact with cm-scale objects frequently encountered in the real world. The soft interfaces of visuotactile sensors are often made of hyperelastic elastomers, which are difficult to simulate quickly and accurately when extremely deformed for tactile information. Additionally, many visuotactile sensors that rely on strict internal light conditions or pattern tracking will fail if the surface is highly deformed. In this work, we propose an algorithm that fuses proximity and visuotactile point clouds for contact patch segmentation that is entirely independent from membrane mechanics. This algorithm exploits the synchronous, high-res proximity and visuotactile modalities enabled by an extremely deformable, selectively transmissive soft membrane, which uses visible light for visuotactile sensing and infrared light for proximity depth. We present the hardware design, membrane fabrication, and evaluation of our contact patch algorithm in low (10%), medium (60%), and high (100%+) membrane strain states. We compare our algorithm against three baselines: proximity-only, tactile-only, and a membrane mechanics model. Our proposed algorithm outperforms all baselines with an average RMSE under 2.8mm of the contact patch geometry across all strain ranges. We demonstrate our contact patch algorithm in four applications: varied stiffness membranes, torque and shear-induced wrinkling, closed loop control for whole body manipulation, and pose estimation. 
Proximity and Visuotactile Point Cloud Fusion for Contact Patches in Extreme Deformation
 - _**标题**_: 极端变形接触斑块的接近和视觉触觉点云融合
- _**摘要**_: 为机器人配备触觉对于在现实世界的操作任务中模拟人类的能力至关重要。视觉触觉传感器是一种流行的触觉传感策略，因为其数据输出与计算机视觉算法兼容，并且可以对局部物体几何形状进行准确、高分辨率的估计。然而，这些传感器在物体交互过程中难以适应传感表面的高变形，从而阻碍了与现实世界中经常遇到的厘米级物体的更多信息接触。视觉触觉传感器的软界面通常由超弹性弹性体制成，当触觉信息极度变形时，很难快速准确地模拟。此外，如果表面高度变形，许多依赖严格内部光线条件或图案跟踪的视觉触觉传感器将会失效。在这项工作中，我们提出了一种融合接近度和视觉触觉点云的算法，用于完全独立于膜力学的接触斑块分割。该算法利用高度可变形、选择性透射软膜实现的同步、高分辨率接近和视觉触觉模式，该软膜使用可见光进行视觉触觉传感，使用红外光进行接近深度。我们介绍了硬件设计、膜制造以及在低 (10%)、中 (60%) 和高 (100%+) 膜应变状态下的接触贴片算法的评估。我们将我们的算法与三个基线进行比较：仅接近、仅触觉和膜力学模型。我们提出的算法优于所有基线，在所有应变范围内接触面几何形状的平均 RMSE 低于 2.8 毫米。我们在四种应用中展示了我们的接触贴片算法：不同刚度的膜、扭矩和剪切引起的起皱、全身操纵的闭环控制以及姿势估计。 
- _**url**_: http://arxiv.org/abs/2307.03839v1


## Paper 16 : Polybot: Training One Policy Across Robots While Embracing Variability
- _**Keywords**_: manipulation, robot
- _**Abstract**_: Reusing large datasets is crucial to scale vision-based robotic manipulators to everyday scenarios due to the high cost of collecting robotic datasets. However, robotic platforms possess varying control schemes, camera viewpoints, kinematic configurations, and end-effector morphologies, posing significant challenges when transferring manipulation skills from one platform to another. To tackle this problem, we propose a set of key design decisions to train a single policy for deployment on multiple robotic platforms. Our framework first aligns the observation and action spaces of our policy across embodiments via utilizing wrist cameras and a unified, but modular codebase. To bridge the remaining domain shift, we align our policy's internal representations across embodiments through contrastive learning. We evaluate our method on a dataset collected over 60 hours spanning 6 tasks and 3 robots with varying joint configurations and sizes: the WidowX 250S, the Franka Emika Panda, and the Sawyer. Our results demonstrate significant improvements in success rate and sample efficiency for our policy when using new task data collected on a different robot, validating our proposed design decisions. More details and videos can be found on our anonymized project website: https://sites.google.com/view/polybot-multirobot 
Polybot: Training One Policy Across Robots While Embracing Variability
 - _**标题**_: Polybot：跨机器人训练一种策略，同时拥抱可变性
- _**摘要**_: 由于收集机器人数据集的成本很高，重用大型数据集对于将基于视觉的机器人操纵器扩展到日常场景至关重要。然而，机器人平台拥有不同的控制方案、摄像机视角、运动学配置和末端执行器形态，在将操作技能从一个平台转移到另一个平台时提出了重大挑战。为了解决这个问题，我们提出了一组关键的设计决策来训练单个策略以在多个机器人平台上部署。我们的框架首先通过利用腕式摄像头和统一但模块化的代码库，跨实施例调整我们策略的观察和行动空间。为了弥合剩余的领域转变，我们通过对比学习来跨实施例调整我们的策略的内部表示。我们在 60 多个小时收集的数据集上评估了我们的方法，该数据集涉及 6 个任务和 3 个具有不同关节配置和尺寸的机器人：WidowX 250S、Franka Emika Panda 和 Sawyer。我们的结果表明，当使用在不同机器人上收集的新任务数据时，我们的策略的成功率和样本效率显着提高，验证了我们提出的设计决策。更多详细信息和视频可以在我们的匿名项目网站上找到：https://sites.google.com/view/polybot-multirobot 
- _**url**_: http://arxiv.org/abs/2307.03719v1


## Paper 17 : Learning Hierarchical Interactive Multi-Object Search for Mobile Manipulation
- _**Keywords**_: manipulation, robot
- _**Abstract**_: Existing object-search approaches enable robots to search through free pathways, however, robots operating in unstructured human-centered environments frequently also have to manipulate the environment to their needs. In this work, we introduce a novel interactive multi-object search task in which a robot has to open doors to navigate rooms and search inside cabinets and drawers to find target objects. These new challenges require combining manipulation and navigation skills in unexplored environments. We present HIMOS, a hierarchical reinforcement learning approach that learns to compose exploration, navigation, and manipulation skills. To achieve this, we design an abstract high-level action space around a semantic map memory and leverage the explored environment as instance navigation points. We perform extensive experiments in simulation and the real-world that demonstrate that HIMOS effectively transfers to new environments in a zero-shot manner. It shows robustness to unseen subpolicies, failures in their execution, and different robot kinematics. These capabilities open the door to a wide range of downstream tasks across embodied AI and real-world use cases. 
Learning Hierarchical Interactive Multi-Object Search for Mobile Manipulation
 - _**标题**_: 学习移动操作的分层交互式多对象搜索
- _**摘要**_: 现有的对象搜索方法使机器人能够搜索自由路径，但是，在非结构化的以人为中心的环境中运行的机器人也经常必须根据其需要操纵环境。在这项工作中，我们介绍了一种新颖的交互式多对象搜索任务，其中机器人必须打开门来导航房间并搜索橱柜和抽屉内部以找到目标对象。这些新的挑战需要在未探索的环境中结合操纵和导航技能。我们提出了 HIMOS，这是一种分层强化学习方法，可以学习组合探索、导航和操作技能。为了实现这一目标，我们围绕语义地图内存设计了一个抽象的高级动作空间，并利用探索的环境作为实例导航点。我们在模拟和现实世界中进行了广泛的实验，证明 HIMOS 以零样本的方式有效地转移到新环境。它显示了对看不见的子策略、执行失败以及不同机器人运动学的鲁棒性。这些功能为跨具体人工智能和现实用例的广泛下游任务打开了大门。 
- _**url**_: http://arxiv.org/abs/2307.06125v1


## Paper 18 : Learning Fine Pinch-Grasp Skills using Tactile Sensing from Real Demonstration Data
- _**Keywords**_: manipulation, robot
- _**Abstract**_: This work develops a data-efficient learning from demonstration framework which exploits the use of rich tactile sensing and achieves fine dexterous bimanual manipulation. Specifically, we formulated a convolutional autoencoder network that can effectively extract and encode high-dimensional tactile information. Further, we developed a behaviour cloning network that can learn human-like sensorimotor skills demonstrated directly on the robot hardware in the task space by fusing both proprioceptive and tactile feedback. Our comparison study with the baseline method revealed the effectiveness of the contact information, which enabled successful extraction and replication of the demonstrated motor skills. Extensive experiments on real dual-arm robots demonstrated the robustness and effectiveness of the fine pinch grasp policy directly learned from one-shot demonstration, including grasping of the same object with different initial poses, generalizing to ten unseen new objects, robust and firm grasping against external pushes, as well as contact-aware and reactive re-grasping in case of dropping objects under very large perturbations. Moreover, the saliency map method is employed to describe the weight distribution across various modalities during pinch grasping. The video is available online at: \href{https://youtu.be/4Pg29bUBKqs}{https://youtu.be/4Pg29bUBKqs}. 
Learning Fine Pinch-Grasp Skills using Tactile Sensing from Real Demonstration Data
 - _**标题**_: 使用真实演示数据中的触觉感知学习精细的捏握技能
- _**摘要**_: 这项工作开发了一种数据高效的学习演示框架，该框架利用丰富的触觉感知并实现精细灵巧的双手操作。具体来说，我们制定了一个卷积自动编码器网络，可以有效地提取和编码高维触觉信息。此外，我们开发了一种行为克隆网络，可以通过融合本体感觉和触觉反馈来学习直接在任务空间中的机器人硬件上展示的类人感觉运动技能。我们与基线方法的比较研究揭示了联系信息的有效性，这使得成功提取和复制所展示的运动技能成为可能。在真实双臂机器人上进行的大量实验证明了直接从一次性演示中学习的精细捏抓抓取策略的鲁棒性和有效性，包括以不同的初始姿势抓取同一物体，推广到十个未见过的新物体，稳健而牢固的抓取外部推动，以及接触感知和反应性重新抓取，以防在非常大的扰动下掉落物体。此外，采用显着图方法来描述捏握过程中各种模态的权重分布。该视频可在线观看：\href{https://youtu.be/4Pg29bUBKqs}{https://youtu.be/4Pg29bUBKqs}。 
- _**url**_: http://arxiv.org/abs/2307.04619v1


## Paper 19 : Giving Robots a Hand: Learning Generalizable Manipulation with Eye-in-Hand Human Video Demonstrations
- _**Keywords**_: manipulation, robot
- _**Abstract**_: Eye-in-hand cameras have shown promise in enabling greater sample efficiency and generalization in vision-based robotic manipulation. However, for robotic imitation, it is still expensive to have a human teleoperator collect large amounts of expert demonstrations with a real robot. Videos of humans performing tasks, on the other hand, are much cheaper to collect since they eliminate the need for expertise in robotic teleoperation and can be quickly captured in a wide range of scenarios. Therefore, human video demonstrations are a promising data source for learning generalizable robotic manipulation policies at scale. In this work, we augment narrow robotic imitation datasets with broad unlabeled human video demonstrations to greatly enhance the generalization of eye-in-hand visuomotor policies. Although a clear visual domain gap exists between human and robot data, our framework does not need to employ any explicit domain adaptation method, as we leverage the partial observability of eye-in-hand cameras as well as a simple fixed image masking scheme. On a suite of eight real-world tasks involving both 3-DoF and 6-DoF robot arm control, our method improves the success rates of eye-in-hand manipulation policies by 58% (absolute) on average, enabling robots to generalize to both new environment configurations and new tasks that are unseen in the robot demonstration data. See video results at https://giving-robots-a-hand.github.io/ . 
Giving Robots a Hand: Learning Generalizable Manipulation with Eye-in-Hand Human Video Demonstrations
 - _**标题**_: 为机器人提供帮助：通过手眼人类视频演示学习通用操作
- _**摘要**_: 手眼相机已显示出在基于视觉的机器人操作中实现更高的样本效率和泛化的前景。然而，对于机器人模仿来说，让人类远程操作员用真实的机器人收集大量的专家演示仍然是昂贵的。另一方面，收集人类执行任务的视频要便宜得多，因为它们不需要机器人远程操作的专业知识，并且可以在各种场景中快速捕获。因此，人类视频演示是大规模学习通用机器人操作策略的有前途的数据源。在这项工作中，我们通过广泛的未标记人类视频演示来增强狭窄的机器人模仿数据集，以极大地增强手眼视觉运动策略的泛化。尽管人类和机器人数据之间存在明显的视觉域差距，但我们的框架不需要采用任何显式域适应方法，因为我们利用了手眼相机的部分可观察性以及简单的固定图像遮罩方案。在涉及 3-DoF 和 6-DoF 机器人手臂控制的一组八个现实世界任务中，我们的方法将手眼操纵策略的成功率平均提高了 58%（绝对），使机器人能够泛化到机器人演示数据中未见的新环境配置和新任务。请访问 https://giving-robots-a-hand.github.io/ 查看视频结果。 
- _**url**_: http://arxiv.org/abs/2307.05959v1


## Paper 20 : GVCCI: Lifelong Learning of Visual Grounding for Language-Guided Robotic Manipulation
- _**Keywords**_: manipulation, robot
- _**Abstract**_: Language-Guided Robotic Manipulation (LGRM) is a challenging task as it requires a robot to understand human instructions to manipulate everyday objects. Recent approaches in LGRM rely on pre-trained Visual Grounding (VG) models to detect objects without adapting to manipulation environments. This results in a performance drop due to a substantial domain gap between the pre-training and real-world data. A straightforward solution is to collect additional training data, but the cost of human-annotation is extortionate. In this paper, we propose Grounding Vision to Ceaselessly Created Instructions (GVCCI), a lifelong learning framework for LGRM, which continuously learns VG without human supervision. GVCCI iteratively generates synthetic instruction via object detection and trains the VG model with the generated data. We validate our framework in offline and online settings across diverse environments on different VG models. Experimental results show that accumulating synthetic data from GVCCI leads to a steady improvement in VG by up to 56.7% and improves resultant LGRM by up to 29.4%. Furthermore, the qualitative analysis shows that the unadapted VG model often fails to find correct objects due to a strong bias learned from the pre-training data. Finally, we introduce a novel VG dataset for LGRM, consisting of nearly 252k triplets of image-object-instruction from diverse manipulation environments. 
GVCCI: Lifelong Learning of Visual Grounding for Language-Guided Robotic Manipulation
 - _**标题**_: GVCCI：语言引导机器人操作的视觉基础的终身学习
- _**摘要**_: 语言引导机器人操作（LGRM）是一项具有挑战性的任务，因为它需要机器人理解人类指令来操作日常物体。 LGRM 中的最新方法依赖于预先训练的视觉接地（VG）模型来检测物体，而不需要适应操作环境。由于预训练和实际数据之间存在巨大的域差距，这会导致性能下降。一个简单的解决方案是收集额外的训练数据，但人工注释的成本过高。在本文中，我们提出了“不断创建指令的基础视觉”（GVCCI），这是一种 LGRM 的终身学习框架，可以在没有人类监督的情况下持续学习 VG。 GVCCI 通过对象检测迭代生成合成指令，并使用生成的数据训练 VG 模型。我们在不同环境下、不同 VG 模型上的离线和在线设置中验证了我们的框架。实验结果表明，积累 GVCCI 的合成数据可使 VG 稳定提高高达 56.7%，并将所得 LGRM 提高高达 29.4%。此外，定性分析表明，由于从预训练数据中学到的强烈偏差，未适应的 VG 模型经常无法找到正确的对象。最后，我们为 LGRM 引入了一个新颖的 VG 数据集，由来自不同操作环境的近 252k 个图像-对象-指令三元组组成。 
- _**url**_: http://arxiv.org/abs/2307.05963v1


## Paper 21 : Decomposing the Generalization Gap in Imitation Learning for Visual Robotic Manipulation
- _**Keywords**_: manipulation, robot
- _**Abstract**_: What makes generalization hard for imitation learning in visual robotic manipulation? This question is difficult to approach at face value, but the environment from the perspective of a robot can often be decomposed into enumerable factors of variation, such as the lighting conditions or the placement of the camera. Empirically, generalization to some of these factors have presented a greater obstacle than others, but existing work sheds little light on precisely how much each factor contributes to the generalization gap. Towards an answer to this question, we study imitation learning policies in simulation and on a real robot language-conditioned manipulation task to quantify the difficulty of generalization to different (sets of) factors. We also design a new simulated benchmark of 19 tasks with 11 factors of variation to facilitate more controlled evaluations of generalization. From our study, we determine an ordering of factors based on generalization difficulty, that is consistent across simulation and our real robot setup. 
Decomposing the Generalization Gap in Imitation Learning for Visual Robotic Manipulation
 - _**标题**_: 分解视觉机器人操作模仿学习中的泛化差距
- _**摘要**_: 是什么使得视觉机器人操作中的模仿学习难以泛化？从表面上看这个问题很难回答，但从机器人的角度来看，环境通常可以分解为无数的变化因素，例如照明条件或相机的放置位置。根据经验，对其中一些因素的泛化比其他因素提出了更大的障碍，但现有的工作几乎没有阐明每个因素对泛化差距的具体贡献有多大。为了回答这个问题，我们研究了模拟中的模仿学习策略和真实的机器人语言条件操作任务，以量化泛化到不同（组）因素的难度。我们还设计了一个包含 19 个任务和 11 个变异因素的新模拟基准，以促进更受控的泛化评估。根据我们的研究，我们根据泛化难度确定因素的排序，这在模拟和我们的真实机器人设置中是一致的。 
- _**url**_: http://arxiv.org/abs/2307.03659v1


## Paper 22 : BiRP: Learning Robot Generalized Bimanual Coordination using Relative Parameterization Method on Human Demonstration
- _**Keywords**_: manipulation, robot
- _**Abstract**_: Human bimanual manipulation can perform more complex tasks than a simple combination of two single arms, which is credited to the spatio-temporal coordination between the arms. However, the description of bimanual coordination is still an open topic in robotics. This makes it difficult to give an explainable coordination paradigm, let alone applied to robotics. In this work, we divide the main bimanual tasks in human daily activities into two types: leader-follower and synergistic coordination. Then we propose a relative parameterization method to learn these types of coordination from human demonstration. It represents coordination as Gaussian mixture models from bimanual demonstration to describe the change in the importance of coordination throughout the motions by probability. The learned coordinated representation can be generalized to new task parameters while ensuring spatio-temporal coordination. We demonstrate the method using synthetic motions and human demonstration data and deploy it to a humanoid robot to perform a generalized bimanual coordination motion. We believe that this easy-to-use bimanual learning from demonstration (LfD) method has the potential to be used as a data augmentation plugin for robot large manipulation model training. The corresponding codes are open-sourced in https://github.com/Skylark0924/Rofunc. 
BiRP: Learning Robot Generalized Bimanual Coordination using Relative Parameterization Method on Human Demonstration
 - _**标题**_: BiRP：使用相对参数化方法进行人体演示的学习机器人广义双手协调
- _**摘要**_: 人类的双手操控可以比两个单臂的简单组合执行更复杂的任务，这要归功于手臂之间的时空协调。然而，双手协调的描述仍然是机器人学中的一个开放话题。这使得很难给出一个可解释的协调范式，更不用说应用于机器人技术了。在这项工作中，我们将人类日常活动中的主要双手任务分为两种类型：领导-跟随和协同协调。然后我们提出了一种相对参数化方法来从人类演示中学习这些类型的协调。它将协调表示为来自双手演示的高斯混合模型，以概率描述整个运动中协调重要性的变化。学习到的协调表示可以推广到新的任务参数，同时确保时空协调。我们使用合成运动和人体演示数据演示了该方法，并将其部署到人形机器人上以执行广义的双手协调运动。我们相信这种易于使用的双手演示学习（LfD）方法有潜力用作机器人大型操纵模型训练的数据增强插件。相应的代码开源于https://github.com/Skylark0924/Rofunc。 
- _**url**_: http://arxiv.org/abs/2307.05933v1


## Paper 23 : Bi-Touch: Bimanual Tactile Manipulation with Sim-to-Real Deep Reinforcement Learning
- _**Keywords**_: manipulation, robot
- _**Abstract**_: Bimanual manipulation with tactile feedback will be key to human-level robot dexterity. However, this topic is less explored than single-arm settings, partly due to the availability of suitable hardware along with the complexity of designing effective controllers for tasks with relatively large state-action spaces. Here we introduce a dual-arm tactile robotic system (Bi-Touch) based on the Tactile Gym 2.0 setup that integrates two affordable industrial-level robot arms with low-cost high-resolution tactile sensors (TacTips). We present a suite of bimanual manipulation tasks tailored towards tactile feedback: bi-pushing, bi-reorienting and bi-gathering. To learn effective policies, we introduce appropriate reward functions for these tasks and propose a novel goal-update mechanism with deep reinforcement learning. We also apply these policies to real-world settings with a tactile sim-to-real approach. Our analysis highlights and addresses some challenges met during the sim-to-real application, e.g. the learned policy tended to squeeze an object in the bi-reorienting task due to the sim-to-real gap. Finally, we demonstrate the generalizability and robustness of this system by experimenting with different unseen objects with applied perturbations in the real world. Code and videos are available at https://sites.google.com/view/bi-touch/. 
Bi-Touch: Bimanual Tactile Manipulation with Sim-to-Real Deep Reinforcement Learning
 - _**标题**_: Bi-Touch：具有拟真深度强化学习的双手触觉操作
- _**摘要**_: 带有触觉反馈的双手操作将是机器人灵巧程度达到人类水平的关键。然而，与单臂设置相比，这个主题的探索较少，部分原因是合适的硬件的可用性以及为具有相对较大的状态动作空间的任务设计有效控制器的复杂性。在这里，我们介绍一种基于 Tactile Gym 2.0 设置的双臂触觉机器人系统 (Bi-Touch)，该系统集成了两个经济实惠的工业级机器人手臂和低成本高分辨率触觉传感器 (TacTips)。我们提出了一套针对触觉反馈量身定制的双手操作任务：双推、双重定向和双聚集。为了学习有效的策略，我们为这些任务引入了适当的奖励函数，并提出了一种具有深度强化学习的新颖的目标更新机制。我们还通过触觉模拟到真实的方法将这些策略应用到现实世界中。我们的分析强调并解决了模拟到真实应用过程中遇到的一些挑战，例如由于模拟与真实的差距，学习的策略倾向于在双向重定向任务中挤压一个对象。最后，我们通过在现实世界中应用扰动对不同的看不见的物体进行实验，证明了该系统的普遍性和鲁棒性。代码和视频可在 https://sites.google.com/view/bi-touch/ 获取。 
- _**url**_: http://arxiv.org/abs/2307.06423v1


## Paper 24 : A Mixed Reality System for Interaction with Heterogeneous Robotic Systems
- _**Keywords**_: manipulation, robot
- _**Abstract**_: The growing spread of robots for service and industrial purposes calls for versatile, intuitive and portable interaction approaches. In particular, in industrial environments, operators should be able to interact with robots in a fast, effective, and possibly effortless manner. To this end, reality enhancement techniques have been used to achieve efficient management and simplify interactions, in particular in manufacturing and logistics processes. Building upon this, in this paper we propose a system based on mixed reality that allows a ubiquitous interface for heterogeneous robotic systems in dynamic scenarios, where users are involved in different tasks and need to interact with different robots. By means of mixed reality, users can interact with a robot through manipulation of its virtual replica, which is always colocated with the user and is extracted when interaction is needed. The system has been tested in a simulated intralogistics setting, where different robots are present and require sporadic intervention by human operators, who are involved in other tasks. In our setting we consider the presence of drones and AGVs with different levels of autonomy, calling for different user interventions. The proposed approach has been validated in virtual reality, considering quantitative and qualitative assessment of performance and user's feedback. 
A Mixed Reality System for Interaction with Heterogeneous Robotic Systems
 - _**标题**_: 用于与异构机器人系统交互的混合现实系统
- _**摘要**_: 用于服务和工业用途的机器人的日益普及需要多功能、直观和便携式的交互方法。特别是在工业环境中，操作员应该能够以快速、有效且可能毫不费力的方式与机器人进行交互。为此，现实增强技术已用于实现高效管理并简化交互，特别是在制造和物流流程中。在此基础上，在本文中，我们提出了一种基于混合现实的系统，该系统允许在动态场景中为异构机器人系统提供无处不在的界面，其中用户参与不同的任务并需要与不同的机器人交互。通过混合现实，用户可以通过操纵虚拟复制品与机器人进行交互，该虚拟复制品始终与用户位于同一位置，并在需要交互时被提取。该系统已经在模拟的内部物流环境中进行了测试，其中存在不同的机器人，并且需要参与其他任务的人类操作员的零星干预。在我们的设置中，我们考虑了具有不同自主级别的无人机和 AGV 的存在，需要不同的用户干预。考虑到性能和用户反馈的定量和定性评估，所提出的方法已在虚拟现实中得到验证。 
- _**url**_: http://arxiv.org/abs/2307.05280v2


## Paper 25 : TRansPose: Large-Scale Multispectral Dataset for Transparent Object
- _**Abstract**_: Transparent objects are encountered frequently in our daily lives, yet recognizing them poses challenges for conventional vision sensors due to their unique material properties, not being well perceived from RGB or depth cameras. Overcoming this limitation, thermal infrared cameras have emerged as a solution, offering improved visibility and shape information for transparent objects. In this paper, we present TRansPose, the first large-scale multispectral dataset that combines stereo RGB-D, thermal infrared (TIR) images, and object poses to promote transparent object research. The dataset includes 99 transparent objects, encompassing 43 household items, 27 recyclable trashes, 29 chemical laboratory equivalents, and 12 non-transparent objects. It comprises a vast collection of 333,819 images and 4,000,056 annotations, providing instance-level segmentation masks, ground-truth poses, and completed depth information. The data was acquired using a FLIR A65 thermal infrared (TIR) camera, two Intel RealSense L515 RGB-D cameras, and a Franka Emika Panda robot manipulator. Spanning 87 sequences, TRansPose covers various challenging real-life scenarios, including objects filled with water, diverse lighting conditions, heavy clutter, non-transparent or translucent containers, objects in plastic bags, and multi-stacked objects. TRansPose dataset can be accessed from the following link: https://sites.google.com/view/transpose-dataset 
TRansPose: Large-Scale Multispectral Dataset for Transparent Object
 - _**标题**_: TRansPose：透明物体的大规模多光谱数据集
- _**摘要**_: 透明物体在我们的日常生活中经常遇到，但由于其独特的材料特性，识别它们给传统视觉传感器带来了挑战，RGB 或深度相机无法很好地感知它们。热红外摄像机克服了这一限制，成为一种解决方案，为透明物体提供更好的可视性和形状信息。在本文中，我们提出了 TRansPose，这是第一个大规模多光谱数据集，它结合了立体 RGB-D、热红外 (TIR) 图像和物体姿态，以促进透明物体研究。该数据集包括 99 个透明物体，其中包括 43 个家居用品、27 个可回收垃圾、29 个化学实验室等效物和 12 个不透明物体。它包含大量 333,819 个图像和 4,000,056 个注释，提供实例级分割掩模、地面实况姿势和完整的深度信息。数据是使用 FLIR A65 热红外 (TIR) 摄像头、两个 Intel RealSense L515 RGB-D 摄像头和 Franka Emika Panda 机器人操纵器采集的。 TRansPose 跨越 87 个序列，涵盖了各种具有挑战性的现实生活场景，包括装满水的物体、不同的照明条件、严重杂乱、不透明或半透明容器、塑料袋中的物体以及多层堆叠的物体。 TRansPose 数据集可以通过以下链接访问：https://sites.google.com/view/transpose-dataset 
- _**url**_: http://arxiv.org/abs/2307.05016v1


## Paper 26 : Robotic surface exploration with vision and tactile sensing for cracks detection and characterisation
- _**Abstract**_: This paper presents a novel algorithm for crack localisation and detection based on visual and tactile analysis via fibre-optics. A finger-shaped sensor based on fibre-optics is employed for the data acquisition to collect data for the analysis and the experiments. To detect the possible locations of cracks a camera is used to scan an environment while running an object detection algorithm. Once the crack is detected, a fully-connected graph is created from a skeletonised version of the crack. A minimum spanning tree is then employed for calculating the shortest path to explore the crack which is then used to develop the motion planner for the robotic manipulator. The motion planner divides the crack into multiple nodes which are then explored individually. Then, the manipulator starts the exploration and performs the tactile data classification to confirm if there is indeed a crack in that location or just a false positive from the vision algorithm. If a crack is detected, also the length, width, orientation and number of branches are calculated. This is repeated until all the nodes of the crack are explored.   In order to validate the complete algorithm, various experiments are performed: comparison of exploration of cracks through full scan and motion planning algorithm, implementation of frequency-based features for crack classification and geometry analysis using a combination of vision and tactile data. From the results of the experiments, it is shown that the proposed algorithm is able to detect cracks and improve the results obtained from vision to correctly classify cracks and their geometry with minimal cost thanks to the motion planning algorithm. 
Robotic surface exploration with vision and tactile sensing for cracks detection and characterisation
 - _**标题**_: 通过视觉和触觉传感进行机器人表面探索，以进行裂纹检测和表征
- _**摘要**_: 本文提出了一种基于光纤视觉和触觉分析的裂纹定位和检测新算法。采用基于光纤的指形传感器进行数据采集，以收集用于分析和实验的数据。为了检测裂缝的可能位置，使用相机在运行对象检测算法的同时扫描环境。一旦检测到裂缝，就会根据裂缝的骨架版本创建全连接图。然后使用最小生成树来计算探索裂缝的最短路径，然后使用该路径开发机器人操纵器的运动规划器。运动规划器将裂缝划分为多个节点，然后单独探索这些节点。然后，机械臂开始探索并执行触觉数据分类，以确认该位置是否确实存在裂缝，或者只是视觉算法的误报。如果检测到裂纹，还会计算长度、宽度、方向和分支数量。重复此操作，直到探索完裂纹的所有节点。为了验证完整的算法，进行了各种实验：通过全扫描和运动规划算法对裂纹探索进行比较，使用视觉和触觉数据相结合的方式实现基于频率的裂纹分类特征和几何分析。实验结果表明，由于运动规划算法，所提出的算法能够检测裂纹并改进从视觉获得的结果，以最小的成本正确分类裂纹及其几何形状。 
- _**url**_: http://arxiv.org/abs/2307.06784v1


## Paper 27 : Kinematically-Decoupled Impedance Control for Fast Object Visual Servoing and Grasping on Quadruped Manipulators
- _**Abstract**_: We propose a control pipeline for SAG (Searching, Approaching, and Grasping) of objects, based on a decoupled arm kinematic chain and impedance control, which integrates image-based visual servoing (IBVS). The kinematic decoupling allows for fast end-effector motions and recovery that leads to robust visual servoing. The whole approach and pipeline can be generalized for any mobile platform (wheeled or tracked vehicles), but is most suitable for dynamically moving quadruped manipulators thanks to their reactivity against disturbances. The compliance of the impedance controller makes the robot safer for interactions with humans and the environment. We demonstrate the performance and robustness of the proposed approach with various experiments on our 140 kg HyQReal quadruped robot equipped with a 7-DoF manipulator arm. The experiments consider dynamic locomotion, tracking under external disturbances, and fast motions of the target object. 
Kinematically-Decoupled Impedance Control for Fast Object Visual Servoing and Grasping on Quadruped Manipulators
 - _**标题**_: 四足机械臂上快速物体视觉伺服和抓取的运动学解耦阻抗控制
- _**摘要**_: 我们提出了一种基于解耦手臂运动链和阻抗控制的物体 SAG（搜索、接近和抓取）控制管道，其中集成了基于图像的视觉伺服（IBVS）。运动解耦允许末端执行器快速运动和恢复，从而实现强大的视觉伺服。整个方法和流程可以推广到任何移动平台（轮式或履带式车辆），但由于其对干扰的反应能力，最适合动态移动的四足机械手。阻抗控制器的合规性使机器人与人类和环境的交互更加安全。我们在配备 7 自由度机械臂的 140 公斤 HyQReal 四足机器人上进行了各种实验，展示了所提出方法的性能和鲁棒性。实验考虑了动态运动、外部干扰下的跟踪以及目标物体的快速运动。 
- _**url**_: http://arxiv.org/abs/2307.04918v1


## Paper 28 : HA-ViD: A Human Assembly Video Dataset for Comprehensive Assembly Knowledge Understanding
- _**Abstract**_: Understanding comprehensive assembly knowledge from videos is critical for futuristic ultra-intelligent industry. To enable technological breakthrough, we present HA-ViD - the first human assembly video dataset that features representative industrial assembly scenarios, natural procedural knowledge acquisition process, and consistent human-robot shared annotations. Specifically, HA-ViD captures diverse collaboration patterns of real-world assembly, natural human behaviors and learning progression during assembly, and granulate action annotations to subject, action verb, manipulated object, target object, and tool. We provide 3222 multi-view, multi-modality videos (each video contains one assembly task), 1.5M frames, 96K temporal labels and 2M spatial labels. We benchmark four foundational video understanding tasks: action recognition, action segmentation, object detection and multi-object tracking. Importantly, we analyze their performance for comprehending knowledge in assembly progress, process efficiency, task collaboration, skill parameters and human intention. Details of HA-ViD is available at: https://iai-hrc.github.io/ha-vid. 
HA-ViD: A Human Assembly Video Dataset for Comprehensive Assembly Knowledge Understanding
 - _**标题**_: HA-ViD：用于全面理解装配知识的人体装配视频数据集
- _**摘要**_: 从视频中了解全面的装配知识对于未来超智能工业至关重要。为了实现技术突破，我们推出了 HA-ViD——第一个人体装配视频数据集，具有代表性的工业装配场景、自然的程序性知识获取过程和一致的人机共享注释。具体来说，HA-ViD 捕获现实世界装配、自然人类行为和装配过程中的学习进度的多种协作模式，并将动作注释细化到主题、动作动词、操纵对象、目标对象和工具。我们提供 3222 个多视图、多模态视频（每个视频包含一个组装任务）、150 万帧、96K 时间标签和 2M 空间标签。我们对四个基本视频理解任务进行了基准测试：动作识别、动作分割、对象检测和多对象跟踪。重要的是，我们分析他们的表现，以理解装配进度、流程效率、任务协作、技能参数和人类意图方面的知识。 HA-ViD 的详细信息请访问：https://iai-hrc.github.io/ha-vid。 
- _**url**_: http://arxiv.org/abs/2307.05721v1


## Paper 29 : Forward Dynamics Estimation from Data-Driven Inverse Dynamics Learning
- _**Abstract**_: In this paper, we propose to estimate the forward dynamics equations of mechanical systems by learning a model of the inverse dynamics and estimating individual dynamics components from it. We revisit the classical formulation of rigid body dynamics in order to extrapolate the physical dynamical components, such as inertial and gravitational components, from an inverse dynamics model. After estimating the dynamical components, the forward dynamics can be computed in closed form as a function of the learned inverse dynamics. We tested the proposed method with several machine learning models based on Gaussian Process Regression and compared them with the standard approach of learning the forward dynamics directly. Results on two simulated robotic manipulators, a PANDA Franka Emika and a UR10, show the effectiveness of the proposed method in learning the forward dynamics, both in terms of accuracy as well as in opening the possibility of using more structured~models. 
Forward Dynamics Estimation from Data-Driven Inverse Dynamics Learning
 - _**标题**_: 数据驱动的逆向动力学学习的正向动力学估计
- _**摘要**_: 在本文中，我们建议通过学习逆动力学模型并从中估计各个动力学分量来估计机械系统的正向动力学方程。我们重新审视刚体动力学的经典公式，以便从逆动力学模型中推断出物理动力学分量，例如惯性和重力分量。在估计动态分量之后，可以以封闭形式计算正向动态作为学习的逆动态的函数。我们使用几种基于高斯过程回归的机器学习模型测试了所提出的方法，并将它们与直接学习前向动力学的标准方法进行了比较。两个模拟机器人操纵器（PANDA Franka Emika 和 UR10）的结果显示了所提出的方法在学习前向动力学方面的有效性，无论是在准确性方面还是在开启使用更多结构化模型的可能性方面。 
- _**url**_: http://arxiv.org/abs/2307.05093v1


## Paper 30 : Adaptive Compliant Robot Control with Failure Recovery for Object Press-Fitting
- _**Abstract**_: Loading of shipping containers for dairy products often includes a press-fit task, which involves manually stacking milk cartons in a container without using pallets or packaging. Automating this task with a mobile manipulator can reduce worker strain, and also enhance the efficiency and safety of the container loading process. This paper proposes an approach called Adaptive Compliant Control with Integrated Failure Recovery (ACCIFR), which enables a mobile manipulator to reliably perform the press-fit task. We base the approach on a demonstration learning-based compliant control framework, such that we integrate a monitoring and failure recovery mechanism for successful task execution. Concretely, we monitor the execution through distance and force feedback, detect collisions while the robot is performing the press-fit task, and use wrench measurements to classify the direction of collision; this information informs the subsequent recovery process. We evaluate the method on a miniature container setup, considering variations in the (i) starting position of the end effector, (ii) goal configuration, and (iii) object grasping position. The results demonstrate that the proposed approach outperforms the baseline demonstration-based learning framework regarding adaptability to environmental variations and the ability to recover from collision failures, making it a promising solution for practical press-fit applications. 
Adaptive Compliant Robot Control with Failure Recovery for Object Press-Fitting
 - _**标题**_: 具有物体压装故障恢复功能的自适应机器人控制
- _**摘要**_: 乳制品运输容器的装载通常包括压装任务，其中涉及在不使用托盘或包装的情况下手动将牛奶盒堆放在容器中。使用移动机械手自动执行此任务可以减轻工人的压力，还可以提高集装箱装载过程的效率和安全性。本文提出了一种称为具有集成故障恢复功能的自适应兼容控制（ACCIFR）的方法，该方法使移动机械手能够可靠地执行压接任务。我们将该方法基于基于演示学习的合规控制框架，以便集成监控和故障恢复机制以成功执行任务。具体来说，我们通过距离和力反馈来监控执行情况，在机器人执行压装任务时检测碰撞，并使用扳手测量对碰撞方向进行分类；此信息告知后续恢复过程。我们在微型容器设置上评估该方法，考虑 (i) 末端执行器的起始位置、(ii) 目标配置和 (iii) 物体抓取位置的变化。结果表明，在对环境变化的适应性和从碰撞故障中恢复的能力方面，所提出的方法优于基于演示的基线学习框架，使其成为实际压接应用的有前途的解决方案。 
- _**url**_: http://arxiv.org/abs/2307.08274v1


## Paper 31 : A Versatile Door Opening System with Mobile Manipulator through Adaptive Position-Force Control and Reinforcement Learning
- _**Abstract**_: The ability of robots to navigate through doors is crucial for their effective operation in indoor environments. Consequently, extensive research has been conducted to develop robots capable of opening specific doors. However, the diverse combinations of door handles and opening directions necessitate a more versatile door opening system for robots to successfully operate in real-world environments. In this paper, we propose a mobile manipulator system that can autonomously open various doors without prior knowledge. By using convolutional neural networks, point cloud extraction techniques, and external force measurements during exploratory motion, we obtained information regarding handle types, poses, and door characteristics. Through two different approaches, adaptive position-force control and deep reinforcement learning, we successfully opened doors without precise trajectory or excessive external force. The adaptive position-force control method involves moving the end-effector in the direction of the door opening while responding compliantly to external forces, ensuring safety and manipulator workspace. Meanwhile, the deep reinforcement learning policy minimizes applied forces and eliminates unnecessary movements, enabling stable operation across doors with different poses and widths. The RL-based approach outperforms the adaptive position-force control method in terms of compensating for external forces, ensuring smooth motion, and achieving efficient speed. It reduces the maximum force required by 3.27 times and improves motion smoothness by 1.82 times. However, the non-learning-based adaptive position-force control method demonstrates more versatility in opening a wider range of doors, encompassing revolute doors with four distinct opening directions and varying widths. 
A Versatile Door Opening System with Mobile Manipulator through Adaptive Position-Force Control and Reinforcement Learning
 - _**标题**_: 通过自适应位置力控制和强化学习，配备移动机械手的多功能开门系统
- _**摘要**_: 机器人穿过门的能力对于它们在室内环境中的有效运行至关重要。因此，人们进行了大量研究来开发能够打开特定门的机器人。然而，门把手和开门方向的多样化组合需要更通用的开门系统，以便机器人在现实环境中成功运行。在本文中，我们提出了一种移动机械手系统，可以在无需先验知识的情况下自主打开各种门。通过使用卷积神经网络、点云提取技术和探索性运动期间的外力测量，我们获得了有关手柄类型、姿势和门特征的信息。通过自适应位置力控制和深度强化学习两种不同的方法，我们在没有精确轨迹或过度外力的情况下成功打开了大门。自适应位置力控制方法包括沿开门方向移动末端执行器，同时对外力做出顺从响应，确保安全和机械手工作空间。同时，深度强化学习策略最大限度地减少施加力并消除不必要的运动，从而实现不同姿势和宽度的门的稳定操作。基于强化学习的方法在补偿外力、确保平稳运动和实现高效速度方面优于自适应位置力控制方法。所需的最大力减少了3.27倍，运动平滑度提高了1.82倍。然而，基于非学习的自适应位置力控制方法在打开更广泛的门方面表现出更多的多功能性，包括具有四个不同打开方向和不同宽度的旋转门。 
- _**url**_: http://arxiv.org/abs/2307.04422v1


