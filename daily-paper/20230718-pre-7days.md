# 2023-07-07 to 2023-07-18 Paper List 

## Paper 1 : Robotic Manipulation Datasets for Offline Compositional Reinforcement Learning
- _**Keywords**_: manipulation, robot, RL
- _**Abstract**_: Offline reinforcement learning (RL) is a promising direction that allows RL agents to pre-train on large datasets, avoiding the recurrence of expensive data collection. To advance the field, it is crucial to generate large-scale datasets. Compositional RL is particularly appealing for generating such large datasets, since 1) it permits creating many tasks from few components, 2) the task structure may enable trained agents to solve new tasks by combining relevant learned components, and 3) the compositional dimensions provide a notion of task relatedness. This paper provides four offline RL datasets for simulated robotic manipulation created using the 256 tasks from CompoSuite [Mendez et al., 2022a]. Each dataset is collected from an agent with a different degree of performance, and consists of 256 million transitions. We provide training and evaluation settings for assessing an agent's ability to learn compositional task policies. Our benchmarking experiments on each setting show that current offline RL methods can learn the training tasks to some extent and that compositional methods significantly outperform non-compositional methods. However, current methods are still unable to extract the tasks' compositional structure to generalize to unseen tasks, showing a need for further research in offline compositional RL. 
Robotic Manipulation Datasets for Offline Compositional Reinforcement Learning
 - _**标题**_: 操纵机器人离线成分强化学习的数据集
- _**摘要**_: 离线强化学习(RL)是一个有前途的方向,允许RL特工pre-train在大型数据集上,避免昂贵的数据收集的复发。 
- _**url**_: http://arxiv.org/abs/2307.07091v1


## Paper 2 : Meta-Policy Learning over Plan Ensembles for Robust Articulated Object Manipulation
- _**Keywords**_: manipulation, robot, RL
- _**Abstract**_: Recent work has shown that complex manipulation skills, such as pushing or pouring, can be learned through state-of-the-art learning based techniques, such as Reinforcement Learning (RL). However, these methods often have high sample-complexity, are susceptible to domain changes, and produce unsafe motions that a robot should not perform. On the other hand, purely geometric model-based planning can produce complex behaviors that satisfy all the geometric constraints of the robot but might not be dynamically feasible for a given environment. In this work, we leverage a geometric model-based planner to build a mixture of path-policies on which a task-specific meta-policy can be learned to complete the task. In our results, we demonstrate that a successful meta-policy can be learned to push a door, while requiring little data and being robust to model uncertainty of the environment. We tested our method on a 7-DOF Franka-Emika Robot pushing a cabinet door in simulation. 
Meta-Policy Learning over Plan Ensembles for Robust Articulated Object Manipulation
 - _**标题**_: Meta-Policy学习计划集合体健壮的操纵的对象
- _**摘要**_: 最近的研究表明,复杂的操作技能,如推动或浇注,可以学到通过先进的学习基础技术,如强化学习(RL)。 
- _**url**_: http://arxiv.org/abs/2307.04040v1


## Paper 3 : Magnetic Field-Based Reward Shaping for Goal-Conditioned Reinforcement Learning
- _**Keywords**_: manipulation, robot, RL
- _**Abstract**_: Goal-conditioned reinforcement learning (RL) is an interesting extension of the traditional RL framework, where the dynamic environment and reward sparsity can cause conventional learning algorithms to fail. Reward shaping is a practical approach to improving sample efficiency by embedding human domain knowledge into the learning process. Existing reward shaping methods for goal-conditioned RL are typically built on distance metrics with a linear and isotropic distribution, which may fail to provide sufficient information about the ever-changing environment with high complexity. This paper proposes a novel magnetic field-based reward shaping (MFRS) method for goal-conditioned RL tasks with dynamic target and obstacles. Inspired by the physical properties of magnets, we consider the target and obstacles as permanent magnets and establish the reward function according to the intensity values of the magnetic field generated by these magnets. The nonlinear and anisotropic distribution of the magnetic field intensity can provide more accessible and conducive information about the optimization landscape, thus introducing a more sophisticated magnetic reward compared to the distance-based setting. Further, we transform our magnetic reward to the form of potential-based reward shaping by learning a secondary potential function concurrently to ensure the optimal policy invariance of our method. Experiments results in both simulated and real-world robotic manipulation tasks demonstrate that MFRS outperforms relevant existing methods and effectively improves the sample efficiency of RL algorithms in goal-conditioned tasks with various dynamics of the target and obstacles. 
Magnetic Field-Based Reward Shaping for Goal-Conditioned Reinforcement Learning
 - _**标题**_: 现场奖励形成磁Goal-Conditioned强化学习
- _**摘要**_: Goal-conditioned强化学习(RL)是一个有趣的扩展传统的RL框架,在动态环境和奖励稀疏可以导致传统的学习算法失败。 
- _**url**_: http://arxiv.org/abs/2307.08033v1


## Paper 4 : Boosting Feedback Efficiency of Interactive Reinforcement Learning by Adaptive Learning from Scores
- _**Keywords**_: manipulation, robot, RL
- _**Abstract**_: Interactive reinforcement learning has shown promise in learning complex robotic tasks. However, the process can be human-intensive due to the requirement of large amount of interactive feedback. This paper presents a new method that uses scores provided by humans, instead of pairwise preferences, to improve the feedback efficiency of interactive reinforcement learning. Our key insight is that scores can yield significantly more data than pairwise preferences. Specifically, we require a teacher to interactively score the full trajectories of an agent to train a behavioral policy in a sparse reward environment. To avoid unstable scores given by human negatively impact the training process, we propose an adaptive learning scheme. This enables the learning paradigm to be insensitive to imperfect or unreliable scores. We extensively evaluate our method on robotic locomotion and manipulation tasks. The results show that the proposed method can efficiently learn near-optimal policies by adaptive learning from scores, while requiring less feedback compared to pairwise preference learning methods. The source codes are publicly available at https://github.com/SSKKai/Interactive-Scoring-IRL. 
Boosting Feedback Efficiency of Interactive Reinforcement Learning by Adaptive Learning from Scores
 - _**标题**_: 提高反馈效率互动强化学习的自适应学习成绩
- _**摘要**_: 互动强化学习在学习复杂的机器人已显示出任务。 
- _**url**_: http://arxiv.org/abs/2307.05405v1


## Paper 5 : Seeing is not Believing: Robust Reinforcement Learning against Spurious Correlation
- _**Keywords**_: manipulation, RL
- _**Abstract**_: Robustness has been extensively studied in reinforcement learning (RL) to handle various forms of uncertainty such as random perturbations, rare events, and malicious attacks. In this work, we consider one critical type of robustness against spurious correlation, where different portions of the state do not have causality but have correlations induced by unobserved confounders. These spurious correlations are ubiquitous in real-world tasks, for instance, a self-driving car usually observes heavy traffic in the daytime and light traffic at night due to unobservable human activity. A model that learns such useless or even harmful correlation could catastrophically fail when the confounder in the test case deviates from the training one. Although motivated, enabling robustness against spurious correlation poses significant challenges since the uncertainty set, shaped by the unobserved confounder and sequential structure of RL, is difficult to characterize and identify. Existing robust algorithms that assume simple and unstructured uncertainty sets are therefore inadequate to address this challenge. To solve this issue, we propose Robust State-Confounded Markov Decision Processes (RSC-MDPs) and theoretically demonstrate its superiority in breaking spurious correlations compared with other robust RL counterparts. We also design an empirical algorithm to learn the robust optimal policy for RSC-MDPs, which outperforms all baselines in eight realistic self-driving and manipulation tasks. 
Seeing is not Believing: Robust Reinforcement Learning against Spurious Correlation
 - _**标题**_: 看到的是不相信:对伪相关健壮的强化学习
- _**摘要**_: 鲁棒性已被广泛研究了强化学习(RL)来处理各种形式的不确定性等随机扰动,罕见的事件,恶意攻击。 
- _**url**_: http://arxiv.org/abs/2307.07907v1


## Paper 6 : VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models
- _**Keywords**_: robot, LLM
- _**Abstract**_: Large language models (LLMs) are shown to possess a wealth of actionable knowledge that can be extracted for robot manipulation in the form of reasoning and planning. Despite the progress, most still rely on pre-defined motion primitives to carry out the physical interactions with the environment, which remains a major bottleneck. In this work, we aim to synthesize robot trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a large variety of manipulation tasks given an open-set of instructions and an open-set of objects. We achieve this by first observing that LLMs excel at inferring affordances and constraints given a free-form language instruction. More importantly, by leveraging their code-writing capabilities, they can interact with a visual-language model (VLM) to compose 3D value maps to ground the knowledge into the observation space of the agent. The composed value maps are then used in a model-based planning framework to zero-shot synthesize closed-loop robot trajectories with robustness to dynamic perturbations. We further demonstrate how the proposed framework can benefit from online experiences by efficiently learning a dynamics model for scenes that involve contact-rich interactions. We present a large-scale study of the proposed method in both simulated and real-robot environments, showcasing the ability to perform a large variety of everyday manipulation tasks specified in free-form natural language. Project website: https://voxposer.github.io 
VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models
 - _**标题**_: VoxPoser:可组合3 d值映射为机器人操纵语言模型
- _**摘要**_: 大型语言模型(llm)显示拥有丰富的可操作的知识,可以提取机器人操纵的形式推理和规划。 
- _**url**_: http://arxiv.org/abs/2307.05973v1


## Paper 7 : SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Task Planning
- _**Keywords**_: robot, LLM
- _**Abstract**_: Large language models (LLMs) have demonstrated impressive results in developing generalist planning agents for diverse tasks. However, grounding these plans in expansive, multi-floor, and multi-room environments presents a significant challenge for robotics. We introduce SayPlan, a scalable approach to LLM-based, large-scale task planning for robotics using 3D scene graph (3DSG) representations. To ensure the scalability of our approach, we: (1) exploit the hierarchical nature of 3DSGs to allow LLMs to conduct a semantic search for task-relevant subgraphs from a smaller, collapsed representation of the full graph; (2) reduce the planning horizon for the LLM by integrating a classical path planner and (3) introduce an iterative replanning pipeline that refines the initial plan using feedback from a scene graph simulator, correcting infeasible actions and avoiding planning failures. We evaluate our approach on two large-scale environments spanning up to 3 floors, 36 rooms and 140 objects, and show that our approach is capable of grounding large-scale, long-horizon task plans from abstract, and natural language instruction for a mobile manipulator robot to execute. 
SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Task Planning
 - _**标题**_: SayPlan:接地大型语言模型为可伸缩的任务计划使用3 d场景图
- _**摘要**_: 大型语言模型(llm)展示了令人印象深刻的成果在发展中多面手规划代理不同的任务。 
- _**url**_: http://arxiv.org/abs/2307.06135v1


## Paper 8 : RoCo: Dialectic Multi-Robot Collaboration with Large Language Models
- _**Keywords**_: robot, LLM
- _**Abstract**_: We propose a novel approach to multi-robot collaboration that harnesses the power of pre-trained large language models (LLMs) for both high-level communication and low-level path planning. Robots are equipped with LLMs to discuss and collectively reason task strategies. They then generate sub-task plans and task space waypoint paths, which are used by a multi-arm motion planner to accelerate trajectory planning. We also provide feedback from the environment, such as collision checking, and prompt the LLM agents to improve their plan and waypoints in-context. For evaluation, we introduce RoCoBench, a 6-task benchmark covering a wide range of multi-robot collaboration scenarios, accompanied by a text-only dataset for agent representation and reasoning. We experimentally demonstrate the effectiveness of our approach -- it achieves high success rates across all tasks in RoCoBench and adapts to variations in task semantics. Our dialog setup offers high interpretability and flexibility -- in real world experiments, we show RoCo easily incorporates human-in-the-loop, where a user can communicate and collaborate with a robot agent to complete tasks together. See project website https://project-roco.github.io for videos and code. 
RoCo: Dialectic Multi-Robot Collaboration with Large Language Models
 - _**标题**_: RoCo:辩证多机器人协作与大型语言模型
- _**摘要**_: 我们提出一种新颖的多机器人协作方法,利用pre-trained大型语言模型的力量(llm)高层交流和低级的路径规划。 
- _**url**_: http://arxiv.org/abs/2307.04738v1


## Paper 9 : Large Language Models as General Pattern Machines
- _**Keywords**_: robot, LLM
- _**Abstract**_: We observe that pre-trained large language models (LLMs) are capable of autoregressively completing complex token sequences -- from arbitrary ones procedurally generated by probabilistic context-free grammars (PCFG), to more rich spatial patterns found in the Abstract Reasoning Corpus (ARC), a general AI benchmark, prompted in the style of ASCII art. Surprisingly, pattern completion proficiency can be partially retained even when the sequences are expressed using tokens randomly sampled from the vocabulary. These results suggest that without any additional training, LLMs can serve as general sequence modelers, driven by in-context learning. In this work, we investigate how these zero-shot capabilities may be applied to problems in robotics -- from extrapolating sequences of numbers that represent states over time to complete simple motions, to least-to-most prompting of reward-conditioned trajectories that can discover and represent closed-loop policies (e.g., a stabilizing controller for CartPole). While difficult to deploy today for real systems due to latency, context size limitations, and compute costs, the approach of using LLMs to drive low-level control may provide an exciting glimpse into how the patterns among words could be transferred to actions. 
Large Language Models as General Pattern Machines
 - _**标题**_: 大型语言模型的一般模式机器
- _**摘要**_: 我们观察到pre-trained大型语言模型(llm)能够完成复杂的令牌序列自回归——从任意的顺序生成的概率上下文无关文法(PCFG),更丰富的空间模式中发现抽象推理语料库(弧),一般AI基准,促使ASCII艺术风格的。 
- _**url**_: http://arxiv.org/abs/2307.04721v1


## Paper 10 : Coupling Large Language Models with Logic Programming for Robust and General Reasoning from Text
- _**Keywords**_: robot, LLM
- _**Abstract**_: While large language models (LLMs), such as GPT-3, appear to be robust and general, their reasoning ability is not at a level to compete with the best models trained for specific natural language reasoning problems. In this study, we observe that a large language model can serve as a highly effective few-shot semantic parser. It can convert natural language sentences into a logical form that serves as input for answer set programs, a logic-based declarative knowledge representation formalism. The combination results in a robust and general system that can handle multiple question-answering tasks without requiring retraining for each new task. It only needs a few examples to guide the LLM's adaptation to a specific task, along with reusable ASP knowledge modules that can be applied to multiple tasks. We demonstrate that this method achieves state-of-the-art performance on several NLP benchmarks, including bAbI, StepGame, CLUTRR, and gSCAN. Additionally, it successfully tackles robot planning tasks that an LLM alone fails to solve. 
Coupling Large Language Models with Logic Programming for Robust and General Reasoning from Text
 - _**标题**_: 耦合大型语言模型和逻辑编程健壮和一般推理从文本
- _**摘要**_: 而大型语言模型(llm),比如GPT-3,似乎是健壮的和一般,他们的推理能力不是水平与最好的特定的自然语言模型训练推理问题。 
- _**url**_: http://arxiv.org/abs/2307.07696v1


## Paper 11 : TransPose: A Transformer-based 6D Object Pose Estimation Network with Depth Refinement
- _**Keywords**_: manipulation, robot
- _**Abstract**_: As demand for robotics manipulation application increases, accurate vision-based 6D pose estimation becomes essential for autonomous operations. Convolutional Neural Networks (CNNs) based approaches for pose estimation have been previously introduced. However, the quest for better performance still persists especially for accurate robotics manipulation. This quest extends to the Agri-robotics domain. In this paper, we propose TransPose, an improved Transformer-based 6D pose estimation with a depth refinement module. The architecture takes in only an RGB image as input with no additional supplementing modalities such as depth or thermal images. The architecture encompasses an innovative lighter depth estimation network that estimates depth from an RGB image using feature pyramid with an up-sampling method. A transformer-based detection network with additional prediction heads is proposed to directly regress the object's centre and predict the 6D pose of the target. A novel depth refinement module is then used alongside the predicted centers, 6D poses and depth patches to refine the accuracy of the estimated 6D pose. We extensively compared our results with other state-of-the-art methods and analysed our results for fruit-picking applications. The results we achieved show that our proposed technique outperforms the other methods available in the literature. 
TransPose: A Transformer-based 6D Object Pose Estimation Network with Depth Refinement
 - _**标题**_: 置:比起一个的基于变压器6 d对象构成估计网络深度细化
- _**摘要**_: 随着机器人操纵应用程序需求增加,准确建立6 d姿势估计成为自主操作的必要条件。 
- _**url**_: http://arxiv.org/abs/2307.05561v1


## Paper 12 : Taming the Panda with Python: A Powerful Duo for Seamless Robotics Programming and Integration
- _**Keywords**_: manipulation, robot
- _**Abstract**_: Franka Emika robots have gained significant popularity in research and education due to their exceptional versatility and advanced capabilities. This work introduces panda-py - a Python interface and framework designed to empower Franka Emika robotics with accessible and efficient programming. The panda-py interface enhances the usability of Franka Emika robots, enabling researchers and educators to interact with them more effectively. By leveraging Python's simplicity and readability, users can quickly grasp the necessary programming concepts for robot control and manipulation. Moreover, integrating panda-py with other widely used Python packages in domains such as computer vision and machine learning amplifies the robot's capabilities. Researchers can seamlessly leverage the vast ecosystem of Python libraries, thereby enabling advanced perception, decision-making, and control functionalities. This compatibility facilitates the efficient development of sophisticated robotic applications, integrating state-of-the-art techniques from diverse domains without the added complexity of ROS. 
Taming the Panda with Python: A Powerful Duo for Seamless Robotics Programming and Integration
 - _**标题**_: 与Python驯服熊猫:一个强大的机器人编程和无缝集成
- _**摘要**_: Franka Emika机器人获得了显著的受欢迎程度研究和教育由于其特殊的多功能性和先进的功能。 
- _**url**_: http://arxiv.org/abs/2307.07633v1


## Paper 13 : Switching Head-Tail Funnel UNITER for Dual Referring Expression Comprehension with Fetch-and-Carry Tasks
- _**Keywords**_: manipulation, robot
- _**Abstract**_: This paper describes a domestic service robot (DSR) that fetches everyday objects and carries them to specified destinations according to free-form natural language instructions. Given an instruction such as "Move the bottle on the left side of the plate to the empty chair," the DSR is expected to identify the bottle and the chair from multiple candidates in the environment and carry the target object to the destination. Most of the existing multimodal language understanding methods are impractical in terms of computational complexity because they require inferences for all combinations of target object candidates and destination candidates. We propose Switching Head-Tail Funnel UNITER, which solves the task by predicting the target object and the destination individually using a single model. Our method is validated on a newly-built dataset consisting of object manipulation instructions and semi photo-realistic images captured in a standard Embodied AI simulator. The results show that our method outperforms the baseline method in terms of language comprehension accuracy. Furthermore, we conduct physical experiments in which a DSR delivers standardized everyday objects in a standardized domestic environment as requested by instructions with referring expressions. The experimental results show that the object grasping and placing actions are achieved with success rates of more than 90%. 
Switching Head-Tail Funnel UNITER for Dual Referring Expression Comprehension with Fetch-and-Carry Tasks
 - _**标题**_: 切换首尾相接漏斗使者为双指表达理解打杂的任务
- _**摘要**_: 介绍国内服务机器人(域),获取日常物品和携带他们指定的目的地根据自由自然语言指令。 
- _**url**_: http://arxiv.org/abs/2307.07166v1


## Paper 14 : SAR: Generalization of Physiological Agility and Dexterity via Synergistic Action Representation
- _**Keywords**_: manipulation, robot
- _**Abstract**_: Learning effective continuous control policies in high-dimensional systems, including musculoskeletal agents, remains a significant challenge. Over the course of biological evolution, organisms have developed robust mechanisms for overcoming this complexity to learn highly sophisticated strategies for motor control. What accounts for this robust behavioral flexibility? Modular control via muscle synergies, i.e. coordinated muscle co-contractions, is considered to be one putative mechanism that enables organisms to learn muscle control in a simplified and generalizable action space. Drawing inspiration from this evolved motor control strategy, we use physiologically accurate human hand and leg models as a testbed for determining the extent to which a Synergistic Action Representation (SAR) acquired from simpler tasks facilitates learning more complex tasks. We find in both cases that SAR-exploiting policies significantly outperform end-to-end reinforcement learning. Policies trained with SAR were able to achieve robust locomotion on a wide set of terrains with high sample efficiency, while baseline approaches failed to learn meaningful behaviors. Additionally, policies trained with SAR on a multiobject manipulation task significantly outperformed (>70% success) baseline approaches (<20% success). Both of these SAR-exploiting policies were also found to generalize zero-shot to out-of-domain environmental conditions, while policies that did not adopt SAR failed to generalize. Finally, we establish the generality of SAR on broader high-dimensional control problems using a robotic manipulation task set and a full-body humanoid locomotion task. To the best of our knowledge, this investigation is the first of its kind to present an end-to-end pipeline for discovering synergies and using this representation to learn high-dimensional continuous control across a wide diversity of tasks. 
SAR: Generalization of Physiological Agility and Dexterity via Synergistic Action Representation
 - _**标题**_: 特别行政区:泛化的生理敏捷性和灵活性通过协同行动表示
- _**摘要**_: 学习有效的连续控制政策在高维系统,包括肌肉骨骼,仍然是一个重大挑战。 
- _**url**_: http://arxiv.org/abs/2307.03716v2


## Paper 15 : Proximity and Visuotactile Point Cloud Fusion for Contact Patches in Extreme Deformation
- _**Keywords**_: manipulation, robot
- _**Abstract**_: Equipping robots with the sense of touch is critical to emulating the capabilities of humans in real world manipulation tasks. Visuotactile sensors are a popular tactile sensing strategy due to data output compatible with computer vision algorithms and accurate, high resolution estimates of local object geometry. However, these sensors struggle to accommodate high deformations of the sensing surface during object interactions, hindering more informative contact with cm-scale objects frequently encountered in the real world. The soft interfaces of visuotactile sensors are often made of hyperelastic elastomers, which are difficult to simulate quickly and accurately when extremely deformed for tactile information. Additionally, many visuotactile sensors that rely on strict internal light conditions or pattern tracking will fail if the surface is highly deformed. In this work, we propose an algorithm that fuses proximity and visuotactile point clouds for contact patch segmentation that is entirely independent from membrane mechanics. This algorithm exploits the synchronous, high-res proximity and visuotactile modalities enabled by an extremely deformable, selectively transmissive soft membrane, which uses visible light for visuotactile sensing and infrared light for proximity depth. We present the hardware design, membrane fabrication, and evaluation of our contact patch algorithm in low (10%), medium (60%), and high (100%+) membrane strain states. We compare our algorithm against three baselines: proximity-only, tactile-only, and a membrane mechanics model. Our proposed algorithm outperforms all baselines with an average RMSE under 2.8mm of the contact patch geometry across all strain ranges. We demonstrate our contact patch algorithm in four applications: varied stiffness membranes, torque and shear-induced wrinkling, closed loop control for whole body manipulation, and pose estimation. 
Proximity and Visuotactile Point Cloud Fusion for Contact Patches in Extreme Deformation
 - _**标题**_: 距离和Visuotactile点云融合在极端变形接触补丁
- _**摘要**_: 装备机器人触觉模拟至关重要的人类在现实操作任务的能力。 
- _**url**_: http://arxiv.org/abs/2307.03839v1


## Paper 16 : Polybot: Training One Policy Across Robots While Embracing Variability
- _**Keywords**_: manipulation, robot
- _**Abstract**_: Reusing large datasets is crucial to scale vision-based robotic manipulators to everyday scenarios due to the high cost of collecting robotic datasets. However, robotic platforms possess varying control schemes, camera viewpoints, kinematic configurations, and end-effector morphologies, posing significant challenges when transferring manipulation skills from one platform to another. To tackle this problem, we propose a set of key design decisions to train a single policy for deployment on multiple robotic platforms. Our framework first aligns the observation and action spaces of our policy across embodiments via utilizing wrist cameras and a unified, but modular codebase. To bridge the remaining domain shift, we align our policy's internal representations across embodiments through contrastive learning. We evaluate our method on a dataset collected over 60 hours spanning 6 tasks and 3 robots with varying joint configurations and sizes: the WidowX 250S, the Franka Emika Panda, and the Sawyer. Our results demonstrate significant improvements in success rate and sample efficiency for our policy when using new task data collected on a different robot, validating our proposed design decisions. More details and videos can be found on our anonymized project website: https://sites.google.com/view/polybot-multirobot 
Polybot: Training One Policy Across Robots While Embracing Variability
 - _**标题**_: Polybot:训练一个政策在机器人而拥抱变化
- _**摘要**_: 重用大型数据集规模至关重要的机器人机械手建立日常场景由于高成本的收集机器人数据集。 
- _**url**_: http://arxiv.org/abs/2307.03719v1


## Paper 17 : Learning Hierarchical Interactive Multi-Object Search for Mobile Manipulation
- _**Keywords**_: manipulation, robot
- _**Abstract**_: Existing object-search approaches enable robots to search through free pathways, however, robots operating in unstructured human-centered environments frequently also have to manipulate the environment to their needs. In this work, we introduce a novel interactive multi-object search task in which a robot has to open doors to navigate rooms and search inside cabinets and drawers to find target objects. These new challenges require combining manipulation and navigation skills in unexplored environments. We present HIMOS, a hierarchical reinforcement learning approach that learns to compose exploration, navigation, and manipulation skills. To achieve this, we design an abstract high-level action space around a semantic map memory and leverage the explored environment as instance navigation points. We perform extensive experiments in simulation and the real-world that demonstrate that HIMOS effectively transfers to new environments in a zero-shot manner. It shows robustness to unseen subpolicies, failures in their execution, and different robot kinematics. These capabilities open the door to a wide range of downstream tasks across embodied AI and real-world use cases. 
Learning Hierarchical Interactive Multi-Object Search for Mobile Manipulation
 - _**标题**_: 学习分层交互式多目标搜索移动操作
- _**摘要**_: 已存在的对象搜索方法使机器人搜索免费的途径,然而,机器人操作在非结构化的以人为中心的环境经常也需要操作环境对他们的需求。 
- _**url**_: http://arxiv.org/abs/2307.06125v1


## Paper 18 : Learning Fine Pinch-Grasp Skills using Tactile Sensing from Real Demonstration Data
- _**Keywords**_: manipulation, robot
- _**Abstract**_: This work develops a data-efficient learning from demonstration framework which exploits the use of rich tactile sensing and achieves fine dexterous bimanual manipulation. Specifically, we formulated a convolutional autoencoder network that can effectively extract and encode high-dimensional tactile information. Further, we developed a behaviour cloning network that can learn human-like sensorimotor skills demonstrated directly on the robot hardware in the task space by fusing both proprioceptive and tactile feedback. Our comparison study with the baseline method revealed the effectiveness of the contact information, which enabled successful extraction and replication of the demonstrated motor skills. Extensive experiments on real dual-arm robots demonstrated the robustness and effectiveness of the fine pinch grasp policy directly learned from one-shot demonstration, including grasping of the same object with different initial poses, generalizing to ten unseen new objects, robust and firm grasping against external pushes, as well as contact-aware and reactive re-grasping in case of dropping objects under very large perturbations. Moreover, the saliency map method is employed to describe the weight distribution across various modalities during pinch grasping. The video is available online at: \href{https://youtu.be/4Pg29bUBKqs}{https://youtu.be/4Pg29bUBKqs}. 
Learning Fine Pinch-Grasp Skills using Tactile Sensing from Real Demonstration Data
 - _**标题**_: 学习好Pinch-Grasp技能使用触觉传感的演示数据
- _**摘要**_: 这项工作发展data-efficient示范学习框架,利用丰富的触觉传感和达到良好的使用灵巧的双手操作。 
- _**url**_: http://arxiv.org/abs/2307.04619v1


## Paper 19 : Giving Robots a Hand: Learning Generalizable Manipulation with Eye-in-Hand Human Video Demonstrations
- _**Keywords**_: manipulation, robot
- _**Abstract**_: Eye-in-hand cameras have shown promise in enabling greater sample efficiency and generalization in vision-based robotic manipulation. However, for robotic imitation, it is still expensive to have a human teleoperator collect large amounts of expert demonstrations with a real robot. Videos of humans performing tasks, on the other hand, are much cheaper to collect since they eliminate the need for expertise in robotic teleoperation and can be quickly captured in a wide range of scenarios. Therefore, human video demonstrations are a promising data source for learning generalizable robotic manipulation policies at scale. In this work, we augment narrow robotic imitation datasets with broad unlabeled human video demonstrations to greatly enhance the generalization of eye-in-hand visuomotor policies. Although a clear visual domain gap exists between human and robot data, our framework does not need to employ any explicit domain adaptation method, as we leverage the partial observability of eye-in-hand cameras as well as a simple fixed image masking scheme. On a suite of eight real-world tasks involving both 3-DoF and 6-DoF robot arm control, our method improves the success rates of eye-in-hand manipulation policies by 58% (absolute) on average, enabling robots to generalize to both new environment configurations and new tasks that are unseen in the robot demonstration data. See video results at https://giving-robots-a-hand.github.io/ . 
Giving Robots a Hand: Learning Generalizable Manipulation with Eye-in-Hand Human Video Demonstrations
 - _**标题**_: 给机器人的手:学习可概括的操纵与Eye-in-Hand人类的视频演示
- _**摘要**_: 在Eye-in-hand相机使更大的样本和推广应用机器人操作效率。 
- _**url**_: http://arxiv.org/abs/2307.05959v1


## Paper 20 : GVCCI: Lifelong Learning of Visual Grounding for Language-Guided Robotic Manipulation
- _**Keywords**_: manipulation, robot
- _**Abstract**_: Language-Guided Robotic Manipulation (LGRM) is a challenging task as it requires a robot to understand human instructions to manipulate everyday objects. Recent approaches in LGRM rely on pre-trained Visual Grounding (VG) models to detect objects without adapting to manipulation environments. This results in a performance drop due to a substantial domain gap between the pre-training and real-world data. A straightforward solution is to collect additional training data, but the cost of human-annotation is extortionate. In this paper, we propose Grounding Vision to Ceaselessly Created Instructions (GVCCI), a lifelong learning framework for LGRM, which continuously learns VG without human supervision. GVCCI iteratively generates synthetic instruction via object detection and trains the VG model with the generated data. We validate our framework in offline and online settings across diverse environments on different VG models. Experimental results show that accumulating synthetic data from GVCCI leads to a steady improvement in VG by up to 56.7% and improves resultant LGRM by up to 29.4%. Furthermore, the qualitative analysis shows that the unadapted VG model often fails to find correct objects due to a strong bias learned from the pre-training data. Finally, we introduce a novel VG dataset for LGRM, consisting of nearly 252k triplets of image-object-instruction from diverse manipulation environments. 
GVCCI: Lifelong Learning of Visual Grounding for Language-Guided Robotic Manipulation
 - _**标题**_: GVCCI:终身学习的视觉接地Language-Guided机械操纵
- _**摘要**_: Language-Guided机器人操纵(LGRM)是一个具有挑战性的任务,因为它需要一个机器人理解人类的指令来操作日常用品。 
- _**url**_: http://arxiv.org/abs/2307.05963v1


## Paper 21 : Decomposing the Generalization Gap in Imitation Learning for Visual Robotic Manipulation
- _**Keywords**_: manipulation, robot
- _**Abstract**_: What makes generalization hard for imitation learning in visual robotic manipulation? This question is difficult to approach at face value, but the environment from the perspective of a robot can often be decomposed into enumerable factors of variation, such as the lighting conditions or the placement of the camera. Empirically, generalization to some of these factors have presented a greater obstacle than others, but existing work sheds little light on precisely how much each factor contributes to the generalization gap. Towards an answer to this question, we study imitation learning policies in simulation and on a real robot language-conditioned manipulation task to quantify the difficulty of generalization to different (sets of) factors. We also design a new simulated benchmark of 19 tasks with 11 factors of variation to facilitate more controlled evaluations of generalization. From our study, we determine an ordering of factors based on generalization difficulty, that is consistent across simulation and our real robot setup. 
Decomposing the Generalization Gap in Imitation Learning for Visual Robotic Manipulation
 - _**标题**_: 分解泛化差距模仿学习视觉机器人操作
- _**摘要**_: 使推广难模仿学习视觉机器人操纵? 
- _**url**_: http://arxiv.org/abs/2307.03659v1


## Paper 22 : BiRP: Learning Robot Generalized Bimanual Coordination using Relative Parameterization Method on Human Demonstration
- _**Keywords**_: manipulation, robot
- _**Abstract**_: Human bimanual manipulation can perform more complex tasks than a simple combination of two single arms, which is credited to the spatio-temporal coordination between the arms. However, the description of bimanual coordination is still an open topic in robotics. This makes it difficult to give an explainable coordination paradigm, let alone applied to robotics. In this work, we divide the main bimanual tasks in human daily activities into two types: leader-follower and synergistic coordination. Then we propose a relative parameterization method to learn these types of coordination from human demonstration. It represents coordination as Gaussian mixture models from bimanual demonstration to describe the change in the importance of coordination throughout the motions by probability. The learned coordinated representation can be generalized to new task parameters while ensuring spatio-temporal coordination. We demonstrate the method using synthetic motions and human demonstration data and deploy it to a humanoid robot to perform a generalized bimanual coordination motion. We believe that this easy-to-use bimanual learning from demonstration (LfD) method has the potential to be used as a data augmentation plugin for robot large manipulation model training. The corresponding codes are open-sourced in https://github.com/Skylark0924/Rofunc. 
BiRP: Learning Robot Generalized Bimanual Coordination using Relative Parameterization Method on Human Demonstration
 - _**标题**_: BiRP:学习机器人广义用双手的协调使用相对参数化方法对人类演示
- _**摘要**_: 人类用双手的操作可以执行更复杂的任务比简单的组合两个单武器,这是归功于之间的时空协调武器。 
- _**url**_: http://arxiv.org/abs/2307.05933v1


## Paper 23 : Bi-Touch: Bimanual Tactile Manipulation with Sim-to-Real Deep Reinforcement Learning
- _**Keywords**_: manipulation, robot
- _**Abstract**_: Bimanual manipulation with tactile feedback will be key to human-level robot dexterity. However, this topic is less explored than single-arm settings, partly due to the availability of suitable hardware along with the complexity of designing effective controllers for tasks with relatively large state-action spaces. Here we introduce a dual-arm tactile robotic system (Bi-Touch) based on the Tactile Gym 2.0 setup that integrates two affordable industrial-level robot arms with low-cost high-resolution tactile sensors (TacTips). We present a suite of bimanual manipulation tasks tailored towards tactile feedback: bi-pushing, bi-reorienting and bi-gathering. To learn effective policies, we introduce appropriate reward functions for these tasks and propose a novel goal-update mechanism with deep reinforcement learning. We also apply these policies to real-world settings with a tactile sim-to-real approach. Our analysis highlights and addresses some challenges met during the sim-to-real application, e.g. the learned policy tended to squeeze an object in the bi-reorienting task due to the sim-to-real gap. Finally, we demonstrate the generalizability and robustness of this system by experimenting with different unseen objects with applied perturbations in the real world. Code and videos are available at https://sites.google.com/view/bi-touch/. 
Bi-Touch: Bimanual Tactile Manipulation with Sim-to-Real Deep Reinforcement Learning
 - _**标题**_: Bi-Touch:用双手的触摸与Sim-to-Real深的强化学习
- _**摘要**_: 用双手的操作与触觉反馈将人类机器人灵巧的关键。 
- _**url**_: http://arxiv.org/abs/2307.06423v1


## Paper 24 : A Mixed Reality System for Interaction with Heterogeneous Robotic Systems
- _**Keywords**_: manipulation, robot
- _**Abstract**_: The growing spread of robots for service and industrial purposes calls for versatile, intuitive and portable interaction approaches. In particular, in industrial environments, operators should be able to interact with robots in a fast, effective, and possibly effortless manner. To this end, reality enhancement techniques have been used to achieve efficient management and simplify interactions, in particular in manufacturing and logistics processes. Building upon this, in this paper we propose a system based on mixed reality that allows a ubiquitous interface for heterogeneous robotic systems in dynamic scenarios, where users are involved in different tasks and need to interact with different robots. By means of mixed reality, users can interact with a robot through manipulation of its virtual replica, which is always colocated with the user and is extracted when interaction is needed. The system has been tested in a simulated intralogistics setting, where different robots are present and require sporadic intervention by human operators, who are involved in other tasks. In our setting we consider the presence of drones and AGVs with different levels of autonomy, calling for different user interventions. The proposed approach has been validated in virtual reality, considering quantitative and qualitative assessment of performance and user's feedback. 
A Mixed Reality System for Interaction with Heterogeneous Robotic Systems
 - _**标题**_: 混合现实系统与异构机器人系统的交互
- _**摘要**_: 日益蔓延的机器人服务和工业用途要求多才多艺,直观和便携式交互方法。 
- _**url**_: http://arxiv.org/abs/2307.05280v2


## Paper 25 : TRansPose: Large-Scale Multispectral Dataset for Transparent Object
- _**Abstract**_: Transparent objects are encountered frequently in our daily lives, yet recognizing them poses challenges for conventional vision sensors due to their unique material properties, not being well perceived from RGB or depth cameras. Overcoming this limitation, thermal infrared cameras have emerged as a solution, offering improved visibility and shape information for transparent objects. In this paper, we present TRansPose, the first large-scale multispectral dataset that combines stereo RGB-D, thermal infrared (TIR) images, and object poses to promote transparent object research. The dataset includes 99 transparent objects, encompassing 43 household items, 27 recyclable trashes, 29 chemical laboratory equivalents, and 12 non-transparent objects. It comprises a vast collection of 333,819 images and 4,000,056 annotations, providing instance-level segmentation masks, ground-truth poses, and completed depth information. The data was acquired using a FLIR A65 thermal infrared (TIR) camera, two Intel RealSense L515 RGB-D cameras, and a Franka Emika Panda robot manipulator. Spanning 87 sequences, TRansPose covers various challenging real-life scenarios, including objects filled with water, diverse lighting conditions, heavy clutter, non-transparent or translucent containers, objects in plastic bags, and multi-stacked objects. TRansPose dataset can be accessed from the following link: https://sites.google.com/view/transpose-dataset 
TRansPose: Large-Scale Multispectral Dataset for Transparent Object
 - _**标题**_: 为透明对象转置:大规模的多光谱数据集
- _**摘要**_: 透明的对象是我们日常生活中经常遇到,但承认他们带来了挑战传统视觉传感器由于其独特的材料特性,不认为从RGB或深度照相机。 
- _**url**_: http://arxiv.org/abs/2307.05016v1


## Paper 26 : Robotic surface exploration with vision and tactile sensing for cracks detection and characterisation
- _**Abstract**_: This paper presents a novel algorithm for crack localisation and detection based on visual and tactile analysis via fibre-optics. A finger-shaped sensor based on fibre-optics is employed for the data acquisition to collect data for the analysis and the experiments. To detect the possible locations of cracks a camera is used to scan an environment while running an object detection algorithm. Once the crack is detected, a fully-connected graph is created from a skeletonised version of the crack. A minimum spanning tree is then employed for calculating the shortest path to explore the crack which is then used to develop the motion planner for the robotic manipulator. The motion planner divides the crack into multiple nodes which are then explored individually. Then, the manipulator starts the exploration and performs the tactile data classification to confirm if there is indeed a crack in that location or just a false positive from the vision algorithm. If a crack is detected, also the length, width, orientation and number of branches are calculated. This is repeated until all the nodes of the crack are explored.   In order to validate the complete algorithm, various experiments are performed: comparison of exploration of cracks through full scan and motion planning algorithm, implementation of frequency-based features for crack classification and geometry analysis using a combination of vision and tactile data. From the results of the experiments, it is shown that the proposed algorithm is able to detect cracks and improve the results obtained from vision to correctly classify cracks and their geometry with minimal cost thanks to the motion planning algorithm. 
Robotic surface exploration with vision and tactile sensing for cracks detection and characterisation
 - _**标题**_: 机器人与视觉和触觉传感表面探索裂缝检测和描述
- _**摘要**_: 本文提出一种新颖的算法基于视觉和触觉的裂纹本地化和检测分析,通过光纤。 
- _**url**_: http://arxiv.org/abs/2307.06784v1


## Paper 27 : Kinematically-Decoupled Impedance Control for Fast Object Visual Servoing and Grasping on Quadruped Manipulators
- _**Abstract**_: We propose a control pipeline for SAG (Searching, Approaching, and Grasping) of objects, based on a decoupled arm kinematic chain and impedance control, which integrates image-based visual servoing (IBVS). The kinematic decoupling allows for fast end-effector motions and recovery that leads to robust visual servoing. The whole approach and pipeline can be generalized for any mobile platform (wheeled or tracked vehicles), but is most suitable for dynamically moving quadruped manipulators thanks to their reactivity against disturbances. The compliance of the impedance controller makes the robot safer for interactions with humans and the environment. We demonstrate the performance and robustness of the proposed approach with various experiments on our 140 kg HyQReal quadruped robot equipped with a 7-DoF manipulator arm. The experiments consider dynamic locomotion, tracking under external disturbances, and fast motions of the target object. 
Kinematically-Decoupled Impedance Control for Fast Object Visual Servoing and Grasping on Quadruped Manipulators
 - _**标题**_: Kinematically-Decoupled阻抗控制快速对象视觉伺服和把握四足动物操纵者
- _**摘要**_: 我们提出一个控制管道凹陷(搜索、接近和把握)的对象,基于解耦的手臂运动链和阻抗控制,于一体的基于图像的视觉伺服(IBVS)。 
- _**url**_: http://arxiv.org/abs/2307.04918v1


## Paper 28 : HA-ViD: A Human Assembly Video Dataset for Comprehensive Assembly Knowledge Understanding
- _**Abstract**_: Understanding comprehensive assembly knowledge from videos is critical for futuristic ultra-intelligent industry. To enable technological breakthrough, we present HA-ViD - the first human assembly video dataset that features representative industrial assembly scenarios, natural procedural knowledge acquisition process, and consistent human-robot shared annotations. Specifically, HA-ViD captures diverse collaboration patterns of real-world assembly, natural human behaviors and learning progression during assembly, and granulate action annotations to subject, action verb, manipulated object, target object, and tool. We provide 3222 multi-view, multi-modality videos (each video contains one assembly task), 1.5M frames, 96K temporal labels and 2M spatial labels. We benchmark four foundational video understanding tasks: action recognition, action segmentation, object detection and multi-object tracking. Importantly, we analyze their performance for comprehending knowledge in assembly progress, process efficiency, task collaboration, skill parameters and human intention. Details of HA-ViD is available at: https://iai-hrc.github.io/ha-vid. 
HA-ViD: A Human Assembly Video Dataset for Comprehensive Assembly Knowledge Understanding
 - _**标题**_: HA-ViD:人工组装视频数据集综合装配知识的理解
- _**摘要**_: 从视频理解综合装配知识对未来ultra-intelligent行业至关重要。 
- _**url**_: http://arxiv.org/abs/2307.05721v1


## Paper 29 : Forward Dynamics Estimation from Data-Driven Inverse Dynamics Learning
- _**Abstract**_: In this paper, we propose to estimate the forward dynamics equations of mechanical systems by learning a model of the inverse dynamics and estimating individual dynamics components from it. We revisit the classical formulation of rigid body dynamics in order to extrapolate the physical dynamical components, such as inertial and gravitational components, from an inverse dynamics model. After estimating the dynamical components, the forward dynamics can be computed in closed form as a function of the learned inverse dynamics. We tested the proposed method with several machine learning models based on Gaussian Process Regression and compared them with the standard approach of learning the forward dynamics directly. Results on two simulated robotic manipulators, a PANDA Franka Emika and a UR10, show the effectiveness of the proposed method in learning the forward dynamics, both in terms of accuracy as well as in opening the possibility of using more structured~models. 
Forward Dynamics Estimation from Data-Driven Inverse Dynamics Learning
 - _**标题**_: 从数据驱动的逆动力学学习前进动态估计
- _**摘要**_: 在本文中,我们提出估计远期学习模型的机械系统动力学方程的逆动力学和估算个体动力学组件。 
- _**url**_: http://arxiv.org/abs/2307.05093v1


## Paper 30 : Adaptive Compliant Robot Control with Failure Recovery for Object Press-Fitting
- _**Abstract**_: Loading of shipping containers for dairy products often includes a press-fit task, which involves manually stacking milk cartons in a container without using pallets or packaging. Automating this task with a mobile manipulator can reduce worker strain, and also enhance the efficiency and safety of the container loading process. This paper proposes an approach called Adaptive Compliant Control with Integrated Failure Recovery (ACCIFR), which enables a mobile manipulator to reliably perform the press-fit task. We base the approach on a demonstration learning-based compliant control framework, such that we integrate a monitoring and failure recovery mechanism for successful task execution. Concretely, we monitor the execution through distance and force feedback, detect collisions while the robot is performing the press-fit task, and use wrench measurements to classify the direction of collision; this information informs the subsequent recovery process. We evaluate the method on a miniature container setup, considering variations in the (i) starting position of the end effector, (ii) goal configuration, and (iii) object grasping position. The results demonstrate that the proposed approach outperforms the baseline demonstration-based learning framework regarding adaptability to environmental variations and the ability to recover from collision failures, making it a promising solution for practical press-fit applications. 
Adaptive Compliant Robot Control with Failure Recovery for Object Press-Fitting
 - _**标题**_: 自适应柔性机器人控制与故障恢复对象Press-Fitting
- _**摘要**_: 装载集装箱的乳制品通常包括一个压配合任务,其中包括手动堆垛牛奶盒没有使用托盘或集装箱包装。 
- _**url**_: http://arxiv.org/abs/2307.08274v1


## Paper 31 : A Versatile Door Opening System with Mobile Manipulator through Adaptive Position-Force Control and Reinforcement Learning
- _**Abstract**_: The ability of robots to navigate through doors is crucial for their effective operation in indoor environments. Consequently, extensive research has been conducted to develop robots capable of opening specific doors. However, the diverse combinations of door handles and opening directions necessitate a more versatile door opening system for robots to successfully operate in real-world environments. In this paper, we propose a mobile manipulator system that can autonomously open various doors without prior knowledge. By using convolutional neural networks, point cloud extraction techniques, and external force measurements during exploratory motion, we obtained information regarding handle types, poses, and door characteristics. Through two different approaches, adaptive position-force control and deep reinforcement learning, we successfully opened doors without precise trajectory or excessive external force. The adaptive position-force control method involves moving the end-effector in the direction of the door opening while responding compliantly to external forces, ensuring safety and manipulator workspace. Meanwhile, the deep reinforcement learning policy minimizes applied forces and eliminates unnecessary movements, enabling stable operation across doors with different poses and widths. The RL-based approach outperforms the adaptive position-force control method in terms of compensating for external forces, ensuring smooth motion, and achieving efficient speed. It reduces the maximum force required by 3.27 times and improves motion smoothness by 1.82 times. However, the non-learning-based adaptive position-force control method demonstrates more versatility in opening a wider range of doors, encompassing revolute doors with four distinct opening directions and varying widths. 
A Versatile Door Opening System with Mobile Manipulator through Adaptive Position-Force Control and Reinforcement Learning
 - _**标题**_: 多用途门开放系统与移动机械手通过自适应Position-Force控制和强化学习
- _**摘要**_: 机器人导航门的能力是至关重要的在室内环境中有效运行。 
- _**url**_: http://arxiv.org/abs/2307.04422v1


