# 2023-07-07 to 2023-07-18 Paper List 

## Paper 1 : Robotic Manipulation Datasets for Offline Compositional Reinforcement Learning
- _**Keywords**_: manipulation, robot, RL
- _**Abstract**_: Offline reinforcement learning (RL) is a promising direction that allows RL agents to pre-train on large datasets, avoiding the recurrence of expensive data collection. To advance the field, it is crucial to generate large-scale datasets. Compositional RL is particularly appealing for generating such large datasets, since 1) it permits creating many tasks from few components, 2) the task structure may enable trained agents to solve new tasks by combining relevant learned components, and 3) the compositional dimensions provide a notion of task relatedness. This paper provides four offline RL datasets for simulated robotic manipulation created using the 256 tasks from CompoSuite [Mendez et al., 2022a]. Each dataset is collected from an agent with a different degree of performance, and consists of 256 million transitions. We provide training and evaluation settings for assessing an agent's ability to learn compositional task policies. Our benchmarking experiments on each setting show that current offline RL methods can learn the training tasks to some extent and that compositional methods significantly outperform non-compositional methods. However, current methods are still unable to extract the tasks' compositional structure to generalize to unseen tasks, showing a need for further research in offline compositional RL. 
Robotic Manipulation Datasets for Offline Compositional Reinforcement Learning
 - _**标题**_: 操纵机器人离线成分强化学习的数据集
- _**摘要**_: 离线强化学习(RL)是一个有前途的方向,允许RL特工pre-train在大型数据集上,避免昂贵的数据收集的复发。推进领域,它产生的大规模数据集是至关重要的。成分RL特别呼吁产生如此大的数据集,因为1)它允许从几个组件,创建许多任务2)任务结构可以使训练有素的特工来解决新任务通过结合相关学习组件,和3)组成维度提供一个任务关联性的概念。本文提供了四个离线模拟机器人操纵的RL数据集,使用256年创建的任务从CompoSuite(门德斯et al ., 2022)。每个数据集都是来自一个代理有不同程度的表现,2.56亿年,由转换。我们提供培训和评估设置评估代理的政策学习能力的任务。每个设置我们的基准测试实验表明,当前离线RL方法可以学习培训任务,在某种程度上和创作方法明显优于non-compositional方法。然而,当前的方法仍无法提取任务的组成结构推广到看不见的任务,显示在离线成分RL需要进一步研究。 
- _**url**_: http://arxiv.org/abs/2307.07091v1


## Paper 2 : Meta-Policy Learning over Plan Ensembles for Robust Articulated Object Manipulation
- _**Keywords**_: manipulation, robot, RL
- _**Abstract**_: Recent work has shown that complex manipulation skills, such as pushing or pouring, can be learned through state-of-the-art learning based techniques, such as Reinforcement Learning (RL). However, these methods often have high sample-complexity, are susceptible to domain changes, and produce unsafe motions that a robot should not perform. On the other hand, purely geometric model-based planning can produce complex behaviors that satisfy all the geometric constraints of the robot but might not be dynamically feasible for a given environment. In this work, we leverage a geometric model-based planner to build a mixture of path-policies on which a task-specific meta-policy can be learned to complete the task. In our results, we demonstrate that a successful meta-policy can be learned to push a door, while requiring little data and being robust to model uncertainty of the environment. We tested our method on a 7-DOF Franka-Emika Robot pushing a cabinet door in simulation. 
Meta-Policy Learning over Plan Ensembles for Robust Articulated Object Manipulation
 - _**标题**_: Meta-Policy学习计划集合体健壮的操纵的对象
- _**摘要**_: 最近的研究表明,复杂的操作技能,如推动或浇注,可以学到通过先进的学习基础技术,如强化学习(RL)。然而,这些方法往往sample-complexity高、易受域变化,产生不安全的运动,机器人不应该执行。另一方面,纯粹的几何模型规划可以产生复杂的行为,满足所有机器人的几何约束,但可能不是可行的对于一个给定的动态环境。在这项工作中,我们利用一个几何模型计划建立一个混合的path-policies特定于任务的meta-policy可以学会完成任务。在我们的结果中,我们表明,一个成功的meta-policy可以学会推一扇门,同时要求小数据和健壮的模型不确定性的环境。我们测试了我们的方法在一个7自由度Franka-Emika仿真机器人推橱柜门。 
- _**url**_: http://arxiv.org/abs/2307.04040v1


## Paper 3 : Magnetic Field-Based Reward Shaping for Goal-Conditioned Reinforcement Learning
- _**Keywords**_: manipulation, robot, RL
- _**Abstract**_: Goal-conditioned reinforcement learning (RL) is an interesting extension of the traditional RL framework, where the dynamic environment and reward sparsity can cause conventional learning algorithms to fail. Reward shaping is a practical approach to improving sample efficiency by embedding human domain knowledge into the learning process. Existing reward shaping methods for goal-conditioned RL are typically built on distance metrics with a linear and isotropic distribution, which may fail to provide sufficient information about the ever-changing environment with high complexity. This paper proposes a novel magnetic field-based reward shaping (MFRS) method for goal-conditioned RL tasks with dynamic target and obstacles. Inspired by the physical properties of magnets, we consider the target and obstacles as permanent magnets and establish the reward function according to the intensity values of the magnetic field generated by these magnets. The nonlinear and anisotropic distribution of the magnetic field intensity can provide more accessible and conducive information about the optimization landscape, thus introducing a more sophisticated magnetic reward compared to the distance-based setting. Further, we transform our magnetic reward to the form of potential-based reward shaping by learning a secondary potential function concurrently to ensure the optimal policy invariance of our method. Experiments results in both simulated and real-world robotic manipulation tasks demonstrate that MFRS outperforms relevant existing methods and effectively improves the sample efficiency of RL algorithms in goal-conditioned tasks with various dynamics of the target and obstacles. 
Magnetic Field-Based Reward Shaping for Goal-Conditioned Reinforcement Learning
 - _**标题**_: 现场奖励形成磁Goal-Conditioned强化学习
- _**摘要**_: Goal-conditioned强化学习(RL)是一个有趣的扩展传统的RL框架,在动态环境和奖励稀疏可以导致传统的学习算法失败。奖励成形是一种实用的方法来提高抽样效率将人类领域知识嵌入到学习过程。现有奖励塑造方法goal-conditioned RL通常是建立在线性和各向同性分布距离度量,它可能无法提供足够的信息与高复杂性不断变化的环境。本文提出了一种新型磁实地奖励塑造(生产商)方法goal-conditioned RL任务与动态目标和障碍。灵感来自磁铁的物理性质,我们认为目标和障碍是永久磁铁,建立奖励函数根据这些磁铁所产生的磁场的强度值。磁场强度的非线性和各向异性分布可以提供更加便利和有利的信息优化景观,因此引入更复杂的磁奖励相比,基于距离的设置。进一步,我们变换磁奖励电势奖励的形式塑造通过学习第二个势函数同时确保最优政策不变性的方法。实验结果在模拟和真实的机器人操纵任务相关证明生产商优于现有方法,有效提高了样本RL算法效率goal-conditioned任务的各种动态目标和障碍。 
- _**url**_: http://arxiv.org/abs/2307.08033v1


## Paper 4 : Boosting Feedback Efficiency of Interactive Reinforcement Learning by Adaptive Learning from Scores
- _**Keywords**_: manipulation, robot, RL
- _**Abstract**_: Interactive reinforcement learning has shown promise in learning complex robotic tasks. However, the process can be human-intensive due to the requirement of large amount of interactive feedback. This paper presents a new method that uses scores provided by humans, instead of pairwise preferences, to improve the feedback efficiency of interactive reinforcement learning. Our key insight is that scores can yield significantly more data than pairwise preferences. Specifically, we require a teacher to interactively score the full trajectories of an agent to train a behavioral policy in a sparse reward environment. To avoid unstable scores given by human negatively impact the training process, we propose an adaptive learning scheme. This enables the learning paradigm to be insensitive to imperfect or unreliable scores. We extensively evaluate our method on robotic locomotion and manipulation tasks. The results show that the proposed method can efficiently learn near-optimal policies by adaptive learning from scores, while requiring less feedback compared to pairwise preference learning methods. The source codes are publicly available at https://github.com/SSKKai/Interactive-Scoring-IRL. 
Boosting Feedback Efficiency of Interactive Reinforcement Learning by Adaptive Learning from Scores
 - _**标题**_: 提高反馈效率互动强化学习的自适应学习成绩
- _**摘要**_: 互动强化学习在学习复杂的机器人已显示出任务。然而,这个过程可以高强度的人工由于大量的互动反馈的要求。本文提出了一种新的方法,使用分数由人类,而不是成对偏好,提高反馈互动强化学习的效率。我们的主要观点是,分数比成对的偏好可以产生更多的数据。具体地说,我们需要一个老师交互式地分代理的完整轨迹训练在稀疏的奖励行为政策环境。为了避免不稳定的得分由人类产生负面影响的培训过程中,我们提出一种自适应学习计划。这使得学习范式对不完善或不可靠的分数。我们广泛的评估我们的方法在机器人运动和操作任务。结果表明,该方法可以有效地学习算法的自适应学习策略得分,同时要求更少的反馈而成对学习方法的偏好。在https://github.com/SSKKai/Interactive-Scoring-IRL源代码是公开的。 
- _**url**_: http://arxiv.org/abs/2307.05405v1


## Paper 5 : Seeing is not Believing: Robust Reinforcement Learning against Spurious Correlation
- _**Keywords**_: manipulation, RL
- _**Abstract**_: Robustness has been extensively studied in reinforcement learning (RL) to handle various forms of uncertainty such as random perturbations, rare events, and malicious attacks. In this work, we consider one critical type of robustness against spurious correlation, where different portions of the state do not have causality but have correlations induced by unobserved confounders. These spurious correlations are ubiquitous in real-world tasks, for instance, a self-driving car usually observes heavy traffic in the daytime and light traffic at night due to unobservable human activity. A model that learns such useless or even harmful correlation could catastrophically fail when the confounder in the test case deviates from the training one. Although motivated, enabling robustness against spurious correlation poses significant challenges since the uncertainty set, shaped by the unobserved confounder and sequential structure of RL, is difficult to characterize and identify. Existing robust algorithms that assume simple and unstructured uncertainty sets are therefore inadequate to address this challenge. To solve this issue, we propose Robust State-Confounded Markov Decision Processes (RSC-MDPs) and theoretically demonstrate its superiority in breaking spurious correlations compared with other robust RL counterparts. We also design an empirical algorithm to learn the robust optimal policy for RSC-MDPs, which outperforms all baselines in eight realistic self-driving and manipulation tasks. 
Seeing is not Believing: Robust Reinforcement Learning against Spurious Correlation
 - _**标题**_: 看到的是不相信:对伪相关健壮的强化学习
- _**摘要**_: 鲁棒性已被广泛研究了强化学习(RL)来处理各种形式的不确定性等随机扰动,罕见的事件,恶意攻击。在这项工作中,我们考虑一个关键类型的鲁棒性和伪相关,不同部分的状态没有因果关系,但相关性引起的难以察觉的混杂因素。这些虚假的相关性是无处不在在实际任务,例如,一个无人驾驶汽车通常观察交通拥挤在白天,晚上光交通由于难以察觉的人类活动。这样的模式,学习无用甚至有害的相关性可能灾难性的失败当测试用例中的“偏离的训练。虽然动机,使鲁棒性对伪相关自不确定性,提出了重大挑战,“由无法连续RL结构,很难描述和识别。现有的鲁棒算法假设简单的和非结构化的不确定性集因此不足以应对这一挑战。为了解决这个问题,我们建议健壮State-Confounded马尔可夫决策过程(RSC-MDPs)和理论上证明其优越性打破虚假的相关性与其他强劲的RL同行相比。我们也设计一个经验学习算法鲁棒最优政策RSC-MDPs,优于所有基线在八个现实的无人驾驶和操作任务。 
- _**url**_: http://arxiv.org/abs/2307.07907v1


## Paper 6 : VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models
- _**Keywords**_: robot, LLM
- _**Abstract**_: Large language models (LLMs) are shown to possess a wealth of actionable knowledge that can be extracted for robot manipulation in the form of reasoning and planning. Despite the progress, most still rely on pre-defined motion primitives to carry out the physical interactions with the environment, which remains a major bottleneck. In this work, we aim to synthesize robot trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a large variety of manipulation tasks given an open-set of instructions and an open-set of objects. We achieve this by first observing that LLMs excel at inferring affordances and constraints given a free-form language instruction. More importantly, by leveraging their code-writing capabilities, they can interact with a visual-language model (VLM) to compose 3D value maps to ground the knowledge into the observation space of the agent. The composed value maps are then used in a model-based planning framework to zero-shot synthesize closed-loop robot trajectories with robustness to dynamic perturbations. We further demonstrate how the proposed framework can benefit from online experiences by efficiently learning a dynamics model for scenes that involve contact-rich interactions. We present a large-scale study of the proposed method in both simulated and real-robot environments, showcasing the ability to perform a large variety of everyday manipulation tasks specified in free-form natural language. Project website: https://voxposer.github.io 
VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models
 - _**标题**_: VoxPoser:可组合3 d值映射为机器人操纵语言模型
- _**摘要**_: 大型语言模型(llm)显示拥有丰富的可操作的知识,可以提取机器人操纵的形式推理和规划。尽管进展,大多数仍然依赖于预定义的运动基元进行物理与环境的相互作用,这仍然是一个主要的瓶颈。在这项工作中,我们的目标是合成机器人轨迹,即。一个密集的6自由度末端执行器路标点序列,进行各种各样的操作任务给定一个开集的指令和一个开集的对象。我们实现这一目标,首先观察llm擅长推理功能和约束自由格式的语言指令。更重要的是,通过利用他们的编码能力,他们可以与视觉语言交互模型(VLM)组成的3 d值映射到地面代理的观测空间的知识。由价值地图然后用于基于模型的规划框架zero-shot合成闭环机器人轨迹动态扰动的鲁棒性。我们进一步证明该框架可以受益于在线体验通过有效地学习场景涉及contact-rich相互作用的动力学模型。我们提出一个大规模的研究该方法的模拟和真正的机器人环境,展示的能力来执行各种各样的日常操作任务中指定的自由格式的自然语言。项目网站:https://voxposer.github.io 
- _**url**_: http://arxiv.org/abs/2307.05973v1


## Paper 7 : SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Task Planning
- _**Keywords**_: robot, LLM
- _**Abstract**_: Large language models (LLMs) have demonstrated impressive results in developing generalist planning agents for diverse tasks. However, grounding these plans in expansive, multi-floor, and multi-room environments presents a significant challenge for robotics. We introduce SayPlan, a scalable approach to LLM-based, large-scale task planning for robotics using 3D scene graph (3DSG) representations. To ensure the scalability of our approach, we: (1) exploit the hierarchical nature of 3DSGs to allow LLMs to conduct a semantic search for task-relevant subgraphs from a smaller, collapsed representation of the full graph; (2) reduce the planning horizon for the LLM by integrating a classical path planner and (3) introduce an iterative replanning pipeline that refines the initial plan using feedback from a scene graph simulator, correcting infeasible actions and avoiding planning failures. We evaluate our approach on two large-scale environments spanning up to 3 floors, 36 rooms and 140 objects, and show that our approach is capable of grounding large-scale, long-horizon task plans from abstract, and natural language instruction for a mobile manipulator robot to execute. 
SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Task Planning
 - _**标题**_: SayPlan:接地大型语言模型为可伸缩的任务计划使用3 d场景图
- _**摘要**_: 大型语言模型(llm)展示了令人印象深刻的成果在发展中多面手规划代理不同的任务。然而,这些计划在膨胀,接地多层和多房间的环境为机器人提供了一个重大的挑战。LLM-based的可伸缩的方法,我们引入SayPlan大规模任务规划机器人使用3 d场景图(3 dsg)表示。以确保我们的方法的可伸缩性,我们:(1)利用3 dsg的分层特性允许llm进行任务相关的语义搜索从一个规模较小的子图,完全的崩溃表示图;(2)减少LLM通过集成一个经典的规划周期路径规划和(3)引入迭代重新规划管道使用反馈改进初始计划从一个场景图模拟器,纠正不可行的行为,避免计划失败。我们评估我们的方法在两个大型环境生成三层,36个房间和140个对象,并表明我们的方法能够接地大规模、世界范围从抽象的任务计划,和自然语言指令移动机械手机器人来执行。 
- _**url**_: http://arxiv.org/abs/2307.06135v1


## Paper 8 : RoCo: Dialectic Multi-Robot Collaboration with Large Language Models
- _**Keywords**_: robot, LLM
- _**Abstract**_: We propose a novel approach to multi-robot collaboration that harnesses the power of pre-trained large language models (LLMs) for both high-level communication and low-level path planning. Robots are equipped with LLMs to discuss and collectively reason task strategies. They then generate sub-task plans and task space waypoint paths, which are used by a multi-arm motion planner to accelerate trajectory planning. We also provide feedback from the environment, such as collision checking, and prompt the LLM agents to improve their plan and waypoints in-context. For evaluation, we introduce RoCoBench, a 6-task benchmark covering a wide range of multi-robot collaboration scenarios, accompanied by a text-only dataset for agent representation and reasoning. We experimentally demonstrate the effectiveness of our approach -- it achieves high success rates across all tasks in RoCoBench and adapts to variations in task semantics. Our dialog setup offers high interpretability and flexibility -- in real world experiments, we show RoCo easily incorporates human-in-the-loop, where a user can communicate and collaborate with a robot agent to complete tasks together. See project website https://project-roco.github.io for videos and code. 
RoCo: Dialectic Multi-Robot Collaboration with Large Language Models
 - _**标题**_: RoCo:辩证多机器人协作与大型语言模型
- _**摘要**_: 我们提出一种新颖的多机器人协作方法,利用pre-trained大型语言模型的力量(llm)高层交流和低级的路径规划。机器人配有llm讨论和共同原因任务策略。然后生成子任务计划和任务空间路径路径,这被多臂机运动规划师用来加速轨迹规划。我们也提供反馈的环境,比如碰撞检查,提示LLM代理语境的改善计划和锚点。进行评估,我们引入RoCoBench 6-task基准覆盖广泛的多机器人协作场景,伴随着代理表示和推理的文本数据集。我们实验证明我们的方法的有效性,它实现了高成功率在所有任务RoCoBench和适应变化任务语义。我们对话框的设置提供高可解释性和灵活性,在现实世界的实验中,我们将展示RoCo容易包含human-in-the-loop,用户可以与机器人交流和合作代理一起完成任务。看到项目网站https://project-roco.github。视频和代码的io。 
- _**url**_: http://arxiv.org/abs/2307.04738v1


## Paper 9 : Large Language Models as General Pattern Machines
- _**Keywords**_: robot, LLM
- _**Abstract**_: We observe that pre-trained large language models (LLMs) are capable of autoregressively completing complex token sequences -- from arbitrary ones procedurally generated by probabilistic context-free grammars (PCFG), to more rich spatial patterns found in the Abstract Reasoning Corpus (ARC), a general AI benchmark, prompted in the style of ASCII art. Surprisingly, pattern completion proficiency can be partially retained even when the sequences are expressed using tokens randomly sampled from the vocabulary. These results suggest that without any additional training, LLMs can serve as general sequence modelers, driven by in-context learning. In this work, we investigate how these zero-shot capabilities may be applied to problems in robotics -- from extrapolating sequences of numbers that represent states over time to complete simple motions, to least-to-most prompting of reward-conditioned trajectories that can discover and represent closed-loop policies (e.g., a stabilizing controller for CartPole). While difficult to deploy today for real systems due to latency, context size limitations, and compute costs, the approach of using LLMs to drive low-level control may provide an exciting glimpse into how the patterns among words could be transferred to actions. 
Large Language Models as General Pattern Machines
 - _**标题**_: 大型语言模型的一般模式机器
- _**摘要**_: 我们观察到pre-trained大型语言模型(llm)能够完成复杂的令牌序列自回归——从任意的顺序生成的概率上下文无关文法(PCFG),更丰富的空间模式中发现抽象推理语料库(弧),一般AI基准,促使ASCII艺术风格的。令人惊讶的是,模式完成水平可以部分保留即使序列使用令牌表示随机取样的词汇。这些结果表明,没有任何额外的培训,llm可以作为一般序列建模者、受语境学习。在这项工作中,我们调查这些zero-shot功能如何被应用到机器人的问题——从推断序列的数字,代表国家在时间来完成简单的动作,least-to-most促使reward-conditioned轨迹可以发现,代表闭环策略(比如,一个稳定控制器CartPole)。虽然很难部署今天实际系统由于延迟,上下文大小限制,和计算成本,利用llm驱动底层控制的方法可以提供一个令人兴奋的方式模式的话可能会被转移到行动中。 
- _**url**_: http://arxiv.org/abs/2307.04721v1


## Paper 10 : Coupling Large Language Models with Logic Programming for Robust and General Reasoning from Text
- _**Keywords**_: robot, LLM
- _**Abstract**_: While large language models (LLMs), such as GPT-3, appear to be robust and general, their reasoning ability is not at a level to compete with the best models trained for specific natural language reasoning problems. In this study, we observe that a large language model can serve as a highly effective few-shot semantic parser. It can convert natural language sentences into a logical form that serves as input for answer set programs, a logic-based declarative knowledge representation formalism. The combination results in a robust and general system that can handle multiple question-answering tasks without requiring retraining for each new task. It only needs a few examples to guide the LLM's adaptation to a specific task, along with reusable ASP knowledge modules that can be applied to multiple tasks. We demonstrate that this method achieves state-of-the-art performance on several NLP benchmarks, including bAbI, StepGame, CLUTRR, and gSCAN. Additionally, it successfully tackles robot planning tasks that an LLM alone fails to solve. 
Coupling Large Language Models with Logic Programming for Robust and General Reasoning from Text
 - _**标题**_: 耦合大型语言模型和逻辑编程健壮和一般推理从文本
- _**摘要**_: 而大型语言模型(llm),比如GPT-3,似乎是健壮的和一般,他们的推理能力不是水平与最好的特定的自然语言模型训练推理问题。在这项研究中,我们观察到一个大语言模型可以作为高效few-shot语义解析器。它可以将自然语言句子转换为一个逻辑形式作为输入对于回答集程序,基于逻辑的陈述性知识表示形式。组合的结果在一个健壮的和一般的系统,可以处理多个自动问答任务为每个新任务而不需要再培训。它只需要几个例子来指导LLM的适应一个特定的任务,随着可重用的ASP知识模块,可以应用于多个任务。我们证明此方法达到最先进的性能在几个NLP基准,包括波斯神的信徒,StepGame CLUTRR, gSCAN。此外,它成功地解决了机器人规划任务,一个独自LLM未能解决。 
- _**url**_: http://arxiv.org/abs/2307.07696v1


## Paper 11 : TransPose: A Transformer-based 6D Object Pose Estimation Network with Depth Refinement
- _**Keywords**_: manipulation, robot
- _**Abstract**_: As demand for robotics manipulation application increases, accurate vision-based 6D pose estimation becomes essential for autonomous operations. Convolutional Neural Networks (CNNs) based approaches for pose estimation have been previously introduced. However, the quest for better performance still persists especially for accurate robotics manipulation. This quest extends to the Agri-robotics domain. In this paper, we propose TransPose, an improved Transformer-based 6D pose estimation with a depth refinement module. The architecture takes in only an RGB image as input with no additional supplementing modalities such as depth or thermal images. The architecture encompasses an innovative lighter depth estimation network that estimates depth from an RGB image using feature pyramid with an up-sampling method. A transformer-based detection network with additional prediction heads is proposed to directly regress the object's centre and predict the 6D pose of the target. A novel depth refinement module is then used alongside the predicted centers, 6D poses and depth patches to refine the accuracy of the estimated 6D pose. We extensively compared our results with other state-of-the-art methods and analysed our results for fruit-picking applications. The results we achieved show that our proposed technique outperforms the other methods available in the literature. 
TransPose: A Transformer-based 6D Object Pose Estimation Network with Depth Refinement
 - _**标题**_: 置:比起一个的基于变压器6 d对象构成估计网络深度细化
- _**摘要**_: 随着机器人操纵应用程序需求增加,准确建立6 d姿势估计成为自主操作的必要条件。基于卷积神经网络(cnn)的方法构成估计曾被介绍。然而,追求更好的性能仍然坚持特别是精确操纵机器人。这个任务延伸到Agri-robotics域。在本文中,我们提出的转置,比起一个改进的基于变压器6 d姿势估计深度优化模块。架构以只有一个RGB图像作为输入,没有额外的补充形式,如深度或热图像。体系结构包括一个创新的轻深度估计网络估计深度使用功能从RGB图像金字塔up-sampling方法。比起一个的基于变压器检测网络直接与额外的预测正面提出回归对象的中心和预测目标的6 d构成。然后使用小说深度优化模块与预测中心,6 d姿势和深度补丁6 d构成改进估计的准确性。我们广泛地将我们的结果与其他水果采摘应用最先进的方法和分析结果。我们获得的结果表明,我们提出的方法优于其他方法在文献中可用。 
- _**url**_: http://arxiv.org/abs/2307.05561v1


## Paper 12 : Taming the Panda with Python: A Powerful Duo for Seamless Robotics Programming and Integration
- _**Keywords**_: manipulation, robot
- _**Abstract**_: Franka Emika robots have gained significant popularity in research and education due to their exceptional versatility and advanced capabilities. This work introduces panda-py - a Python interface and framework designed to empower Franka Emika robotics with accessible and efficient programming. The panda-py interface enhances the usability of Franka Emika robots, enabling researchers and educators to interact with them more effectively. By leveraging Python's simplicity and readability, users can quickly grasp the necessary programming concepts for robot control and manipulation. Moreover, integrating panda-py with other widely used Python packages in domains such as computer vision and machine learning amplifies the robot's capabilities. Researchers can seamlessly leverage the vast ecosystem of Python libraries, thereby enabling advanced perception, decision-making, and control functionalities. This compatibility facilitates the efficient development of sophisticated robotic applications, integrating state-of-the-art techniques from diverse domains without the added complexity of ROS. 
Taming the Panda with Python: A Powerful Duo for Seamless Robotics Programming and Integration
 - _**标题**_: 与Python驯服熊猫:一个强大的机器人编程和无缝集成
- _**摘要**_: Franka Emika机器人获得了显著的受欢迎程度研究和教育由于其特殊的多功能性和先进的功能。这个工作介绍panda-py——一个Python接口和框架旨在赋予Franka Emika机器人访问和高效的编程。panda-py界面增强的可用性Franka Emika机器人,使研究人员和教育工作者更有效地与它们进行交互。通过利用Python的简单性和可读性,用户可以迅速掌握必要的编程概念机器人控制和操纵。此外,集成panda-py与其他广泛使用的Python包在计算机视觉和机器学习等领域增强机器人的能力。研究人员可以无缝地利用Python库的庞大的生态系统,从而使先进的感知,决策和控制功能。这种兼容性促进复杂的机器人应用程序的有效发展,集成来自不同领域的最先进的技术,同时不会增加活性氧的复杂性。 
- _**url**_: http://arxiv.org/abs/2307.07633v1


## Paper 13 : Switching Head-Tail Funnel UNITER for Dual Referring Expression Comprehension with Fetch-and-Carry Tasks
- _**Keywords**_: manipulation, robot
- _**Abstract**_: This paper describes a domestic service robot (DSR) that fetches everyday objects and carries them to specified destinations according to free-form natural language instructions. Given an instruction such as "Move the bottle on the left side of the plate to the empty chair," the DSR is expected to identify the bottle and the chair from multiple candidates in the environment and carry the target object to the destination. Most of the existing multimodal language understanding methods are impractical in terms of computational complexity because they require inferences for all combinations of target object candidates and destination candidates. We propose Switching Head-Tail Funnel UNITER, which solves the task by predicting the target object and the destination individually using a single model. Our method is validated on a newly-built dataset consisting of object manipulation instructions and semi photo-realistic images captured in a standard Embodied AI simulator. The results show that our method outperforms the baseline method in terms of language comprehension accuracy. Furthermore, we conduct physical experiments in which a DSR delivers standardized everyday objects in a standardized domestic environment as requested by instructions with referring expressions. The experimental results show that the object grasping and placing actions are achieved with success rates of more than 90%. 
Switching Head-Tail Funnel UNITER for Dual Referring Expression Comprehension with Fetch-and-Carry Tasks
 - _**标题**_: 切换首尾相接漏斗使者为双指表达理解打杂的任务
- _**摘要**_: 介绍国内服务机器人(域),获取日常物品和携带他们指定的目的地根据自由自然语言指令。给定的指令,比如“移动瓶子左侧板的空椅子上,“DSR预计将从多个候选人确定瓶子和椅子的环境和目标对象到目的地。大多数现有的多通道语言理解方法的计算复杂度是不切实际的,因为它们需要推断所有组合的目标对象候选人和目标候选人。我们建议切换首尾相接漏斗团结,解决任务的预测目标对象和目标单独使用一个模型。验证我们的方法在一个新建的数据集组成的对象操作指令和半写实的图像捕捉到一个标准的体现人工智能模拟器。结果表明,我们的方法优于基准方法的语言理解的准确性。此外,我们进行物理实验的安全域提供了标准化的日常物品在一个标准化的国内环境下应说明所指的表达式。实验结果表明,物体抓取和放置操作的成功率超过90%。 
- _**url**_: http://arxiv.org/abs/2307.07166v1


## Paper 14 : SAR: Generalization of Physiological Agility and Dexterity via Synergistic Action Representation
- _**Keywords**_: manipulation, robot
- _**Abstract**_: Learning effective continuous control policies in high-dimensional systems, including musculoskeletal agents, remains a significant challenge. Over the course of biological evolution, organisms have developed robust mechanisms for overcoming this complexity to learn highly sophisticated strategies for motor control. What accounts for this robust behavioral flexibility? Modular control via muscle synergies, i.e. coordinated muscle co-contractions, is considered to be one putative mechanism that enables organisms to learn muscle control in a simplified and generalizable action space. Drawing inspiration from this evolved motor control strategy, we use physiologically accurate human hand and leg models as a testbed for determining the extent to which a Synergistic Action Representation (SAR) acquired from simpler tasks facilitates learning more complex tasks. We find in both cases that SAR-exploiting policies significantly outperform end-to-end reinforcement learning. Policies trained with SAR were able to achieve robust locomotion on a wide set of terrains with high sample efficiency, while baseline approaches failed to learn meaningful behaviors. Additionally, policies trained with SAR on a multiobject manipulation task significantly outperformed (>70% success) baseline approaches (<20% success). Both of these SAR-exploiting policies were also found to generalize zero-shot to out-of-domain environmental conditions, while policies that did not adopt SAR failed to generalize. Finally, we establish the generality of SAR on broader high-dimensional control problems using a robotic manipulation task set and a full-body humanoid locomotion task. To the best of our knowledge, this investigation is the first of its kind to present an end-to-end pipeline for discovering synergies and using this representation to learn high-dimensional continuous control across a wide diversity of tasks. 
SAR: Generalization of Physiological Agility and Dexterity via Synergistic Action Representation
 - _**标题**_: 特别行政区:泛化的生理敏捷性和灵活性通过协同行动表示
- _**摘要**_: 学习有效的连续控制政策在高维系统,包括肌肉骨骼,仍然是一个重大挑战。在生物进化过程中,生物已经开发出强大的机制来克服这种复杂性学习高度复杂的电机控制策略。是什么导致了这种强大的行为的灵活性?模块化控制通过肌肉的协同效应,即协调肌肉co-contractions,被认为是一个公认的学习机制,使生物肌肉控制在一个简化的和普遍的行动空间。灵感也来自这个进化的电机控制策略,我们用人类生理上准确的手和腿模型作为确定试验台的程度(SAR)协同行动表示从简单的任务获得的有利于学习更复杂的任务。我们发现在这两种情况下,SAR-exploiting政策明显优于端到端强化学习。政策与SAR能够实现健壮的运动训练一组广泛的地形采样效率高,而基线方法未能学到有意义的行为。此外,政策培训,进一步研究多目标操纵任务显著优于与SAR(> 70%)基线方法(< 20%)。这两个SAR-exploiting政策也发现推广zero-shot范围之外的环境条件,虽然政策,没有采用SAR未能推广。最后,我们建立特区更高维控制问题的一般性使用机器人操纵任务集和一个全身人形移动任务。调查我们所知,这是第一个的礼物一个端到端的管道发现协同效应和学习使用这个表示高维连续控制任务的多样性。 
- _**url**_: http://arxiv.org/abs/2307.03716v2


## Paper 15 : Proximity and Visuotactile Point Cloud Fusion for Contact Patches in Extreme Deformation
- _**Keywords**_: manipulation, robot
- _**Abstract**_: Equipping robots with the sense of touch is critical to emulating the capabilities of humans in real world manipulation tasks. Visuotactile sensors are a popular tactile sensing strategy due to data output compatible with computer vision algorithms and accurate, high resolution estimates of local object geometry. However, these sensors struggle to accommodate high deformations of the sensing surface during object interactions, hindering more informative contact with cm-scale objects frequently encountered in the real world. The soft interfaces of visuotactile sensors are often made of hyperelastic elastomers, which are difficult to simulate quickly and accurately when extremely deformed for tactile information. Additionally, many visuotactile sensors that rely on strict internal light conditions or pattern tracking will fail if the surface is highly deformed. In this work, we propose an algorithm that fuses proximity and visuotactile point clouds for contact patch segmentation that is entirely independent from membrane mechanics. This algorithm exploits the synchronous, high-res proximity and visuotactile modalities enabled by an extremely deformable, selectively transmissive soft membrane, which uses visible light for visuotactile sensing and infrared light for proximity depth. We present the hardware design, membrane fabrication, and evaluation of our contact patch algorithm in low (10%), medium (60%), and high (100%+) membrane strain states. We compare our algorithm against three baselines: proximity-only, tactile-only, and a membrane mechanics model. Our proposed algorithm outperforms all baselines with an average RMSE under 2.8mm of the contact patch geometry across all strain ranges. We demonstrate our contact patch algorithm in four applications: varied stiffness membranes, torque and shear-induced wrinkling, closed loop control for whole body manipulation, and pose estimation. 
Proximity and Visuotactile Point Cloud Fusion for Contact Patches in Extreme Deformation
 - _**标题**_: 距离和Visuotactile点云融合在极端变形接触补丁
- _**摘要**_: 装备机器人触觉模拟至关重要的人类在现实操作任务的能力。Visuotactile传感器是一个受欢迎的触觉传感策略由于数据输出兼容计算机视觉算法和精确,高分辨率估计当地的几何对象。然而,这些传感器难以适应高在对象交互感应表面的变形,阻碍更多的信息接触cm-scale对象在现实世界中经常遇到。visuotactile传感器的软接口通常是由超弹性的弹性体,它难以快速、准确地模拟时极其畸形的触觉信息。此外,许多visuotactile传感器,依靠严格的内部光照条件或模式跟踪将会失败如果表面高度变形。在这项工作中,我们提出一种算法,融合邻近和印迹visuotactile点云分割,是完全独立于膜力学。该算法利用同步,高分辨率距离和visuotactile形式通过极其变形,选择性地递送的软膜,利用可见光visuotactile传感和红外线距离深度。给出硬件设计、膜制备和评价我们的接触补丁算法在低(10%),中等(60%),高(+ 100%)膜应变状态。我们比较算法对三种基线:proximity-only tactile-only,膜力学模型。我们的算法优于所有基线平均小于2.8毫米的RMSE印迹几何所有应变范围。我们证明接触补丁算法在四个应用程序:不同刚度膜,扭矩和shear-induced起皱,闭环控制操纵整个身体,姿势估计。 
- _**url**_: http://arxiv.org/abs/2307.03839v1


## Paper 16 : Polybot: Training One Policy Across Robots While Embracing Variability
- _**Keywords**_: manipulation, robot
- _**Abstract**_: Reusing large datasets is crucial to scale vision-based robotic manipulators to everyday scenarios due to the high cost of collecting robotic datasets. However, robotic platforms possess varying control schemes, camera viewpoints, kinematic configurations, and end-effector morphologies, posing significant challenges when transferring manipulation skills from one platform to another. To tackle this problem, we propose a set of key design decisions to train a single policy for deployment on multiple robotic platforms. Our framework first aligns the observation and action spaces of our policy across embodiments via utilizing wrist cameras and a unified, but modular codebase. To bridge the remaining domain shift, we align our policy's internal representations across embodiments through contrastive learning. We evaluate our method on a dataset collected over 60 hours spanning 6 tasks and 3 robots with varying joint configurations and sizes: the WidowX 250S, the Franka Emika Panda, and the Sawyer. Our results demonstrate significant improvements in success rate and sample efficiency for our policy when using new task data collected on a different robot, validating our proposed design decisions. More details and videos can be found on our anonymized project website: https://sites.google.com/view/polybot-multirobot 
Polybot: Training One Policy Across Robots While Embracing Variability
 - _**标题**_: Polybot:训练一个政策在机器人而拥抱变化
- _**摘要**_: 重用大型数据集规模至关重要的机器人机械手建立日常场景由于高成本的收集机器人数据集。然而,机器人平台具有不同的控制方案,相机视点,运动学配置,和末端执行器的形态,构成重大挑战当操作技能从一个平台转移到另一个地方。为了解决这个问题,我们提出一套关键设计决策训练一个政策部署多个机器人平台。我们首先将观察和行动框架空间我们的政策在通过利用手腕的摄像机和一个统一的体现,但模块化的代码库。桥其余领域的转变,我们使我们的政策在通过对比学习体现的内部表示。我们评估我们的方法收集的数据集在60小时跨越6任务和3机器人不同联合配置和尺寸:250年代WidowX的Franka Emika熊猫和索耶。我们的结果演示示例效率显著提高成功率和我们的政策在使用新任务收集的数据在不同的机器人,验证我们提出的设计决策。可以找到更多的细节和视频在我们匿名项目网站:https://sites.google.com/view/polybot-multirobot 
- _**url**_: http://arxiv.org/abs/2307.03719v1


## Paper 17 : Learning Hierarchical Interactive Multi-Object Search for Mobile Manipulation
- _**Keywords**_: manipulation, robot
- _**Abstract**_: Existing object-search approaches enable robots to search through free pathways, however, robots operating in unstructured human-centered environments frequently also have to manipulate the environment to their needs. In this work, we introduce a novel interactive multi-object search task in which a robot has to open doors to navigate rooms and search inside cabinets and drawers to find target objects. These new challenges require combining manipulation and navigation skills in unexplored environments. We present HIMOS, a hierarchical reinforcement learning approach that learns to compose exploration, navigation, and manipulation skills. To achieve this, we design an abstract high-level action space around a semantic map memory and leverage the explored environment as instance navigation points. We perform extensive experiments in simulation and the real-world that demonstrate that HIMOS effectively transfers to new environments in a zero-shot manner. It shows robustness to unseen subpolicies, failures in their execution, and different robot kinematics. These capabilities open the door to a wide range of downstream tasks across embodied AI and real-world use cases. 
Learning Hierarchical Interactive Multi-Object Search for Mobile Manipulation
 - _**标题**_: 学习分层交互式多目标搜索移动操作
- _**摘要**_: 已存在的对象搜索方法使机器人搜索免费的途径,然而,机器人操作在非结构化的以人为中心的环境经常也需要操作环境对他们的需求。在这项工作中,我们介绍一种新的交互式多目标搜索任务,其中一个机器人必须打开大门房间导航和搜索橱柜和抽屉里面找到目标对象。这些新的挑战需要结合操作和导航技术在未知的环境中。我们现在HIMOS分层强化学习方法,学会撰写勘探、导航和操作技能。为了达到这个目标,我们设计一个抽象的高层行动空间语义映射内存和利用探索周围环境实例导航点。我们执行广泛的仿真和实际实验证明HIMOS zero-shot的方式有效地转移到新的环境。看不见的种子策略显示了鲁棒性,在它们的执行失败,不同的机器人运动学。这些功能打开门广泛的下游任务体现人工智能和真实用例。 
- _**url**_: http://arxiv.org/abs/2307.06125v1


## Paper 18 : Learning Fine Pinch-Grasp Skills using Tactile Sensing from Real Demonstration Data
- _**Keywords**_: manipulation, robot
- _**Abstract**_: This work develops a data-efficient learning from demonstration framework which exploits the use of rich tactile sensing and achieves fine dexterous bimanual manipulation. Specifically, we formulated a convolutional autoencoder network that can effectively extract and encode high-dimensional tactile information. Further, we developed a behaviour cloning network that can learn human-like sensorimotor skills demonstrated directly on the robot hardware in the task space by fusing both proprioceptive and tactile feedback. Our comparison study with the baseline method revealed the effectiveness of the contact information, which enabled successful extraction and replication of the demonstrated motor skills. Extensive experiments on real dual-arm robots demonstrated the robustness and effectiveness of the fine pinch grasp policy directly learned from one-shot demonstration, including grasping of the same object with different initial poses, generalizing to ten unseen new objects, robust and firm grasping against external pushes, as well as contact-aware and reactive re-grasping in case of dropping objects under very large perturbations. Moreover, the saliency map method is employed to describe the weight distribution across various modalities during pinch grasping. The video is available online at: \href{https://youtu.be/4Pg29bUBKqs}{https://youtu.be/4Pg29bUBKqs}. 
Learning Fine Pinch-Grasp Skills using Tactile Sensing from Real Demonstration Data
 - _**标题**_: 学习好Pinch-Grasp技能使用触觉传感的演示数据
- _**摘要**_: 这项工作发展data-efficient示范学习框架,利用丰富的触觉传感和达到良好的使用灵巧的双手操作。具体来说,我们制定一个卷积autoencoder网络,可以有效地提取和编码高维触觉信息。进一步,我们开发了一个行为克隆网络,可以直接学习人类感觉运动技能演示了在机器人的硬件在任务空间融合本体感受的和触觉反馈。我们与基线比较研究方法显示联系方式的有效性,使成功提取了运动技能和复制。广泛真实的双臂机器人实验证明细捏把握政策的稳健性和有效性直接从一次性示范,包括同一对象的把握与不同初始姿势,泛化到十看不见的新对象,健壮和公司抓住外部的推动,以及contact-aware反应就是想删除对象在很大的扰动。此外,凸起映射方法来描述重量分布在各种模式在捏抓。这段视频在网上:\ href {https://youtu.be/4Pg29bUBKqs} {https://youtu.be/4Pg29bUBKqs}。 
- _**url**_: http://arxiv.org/abs/2307.04619v1


## Paper 19 : Giving Robots a Hand: Learning Generalizable Manipulation with Eye-in-Hand Human Video Demonstrations
- _**Keywords**_: manipulation, robot
- _**Abstract**_: Eye-in-hand cameras have shown promise in enabling greater sample efficiency and generalization in vision-based robotic manipulation. However, for robotic imitation, it is still expensive to have a human teleoperator collect large amounts of expert demonstrations with a real robot. Videos of humans performing tasks, on the other hand, are much cheaper to collect since they eliminate the need for expertise in robotic teleoperation and can be quickly captured in a wide range of scenarios. Therefore, human video demonstrations are a promising data source for learning generalizable robotic manipulation policies at scale. In this work, we augment narrow robotic imitation datasets with broad unlabeled human video demonstrations to greatly enhance the generalization of eye-in-hand visuomotor policies. Although a clear visual domain gap exists between human and robot data, our framework does not need to employ any explicit domain adaptation method, as we leverage the partial observability of eye-in-hand cameras as well as a simple fixed image masking scheme. On a suite of eight real-world tasks involving both 3-DoF and 6-DoF robot arm control, our method improves the success rates of eye-in-hand manipulation policies by 58% (absolute) on average, enabling robots to generalize to both new environment configurations and new tasks that are unseen in the robot demonstration data. See video results at https://giving-robots-a-hand.github.io/ . 
Giving Robots a Hand: Learning Generalizable Manipulation with Eye-in-Hand Human Video Demonstrations
 - _**标题**_: 给机器人的手:学习可概括的操纵与Eye-in-Hand人类的视频演示
- _**摘要**_: 在Eye-in-hand相机使更大的样本和推广应用机器人操作效率。然而,对于机器人模仿,它仍然是昂贵的人工遥控机器人收集大量的专家示威与一个真正的机器人。视频的人执行任务,另一方面,更便宜的收集,因为他们在机器人遥操作消除需要专业知识,可以快速捕捉到广泛的场景。因此,人类学习的视频演示是一种很有前途的数据源可概括的机器人操纵政策规模。在这项工作中,我们增加狭窄机器人模仿人类与广泛的无标号数据集视频演示大大增强的概括eye-in-hand visuomotor政策。尽管人类和机器人之间存在一个清晰的视觉领域差距数据,我们的框架不需要使用任何明确的领域适应气候变化的方法,我们利用eye-in-hand摄像头的部分可观测性以及一个简单的固定形象屏蔽方案。一套八真实世界的任务包括三自由度和6自由度机械臂的控制,我们的方法改善eye-in-hand操纵策略的成功率58%(绝对)平均,使机器人能够推广到新环境的配置和新任务,都是看不见的机器人演示的数据。在https://giving-robots-a-hand.github看到视频的结果。io /。 
- _**url**_: http://arxiv.org/abs/2307.05959v1


## Paper 20 : GVCCI: Lifelong Learning of Visual Grounding for Language-Guided Robotic Manipulation
- _**Keywords**_: manipulation, robot
- _**Abstract**_: Language-Guided Robotic Manipulation (LGRM) is a challenging task as it requires a robot to understand human instructions to manipulate everyday objects. Recent approaches in LGRM rely on pre-trained Visual Grounding (VG) models to detect objects without adapting to manipulation environments. This results in a performance drop due to a substantial domain gap between the pre-training and real-world data. A straightforward solution is to collect additional training data, but the cost of human-annotation is extortionate. In this paper, we propose Grounding Vision to Ceaselessly Created Instructions (GVCCI), a lifelong learning framework for LGRM, which continuously learns VG without human supervision. GVCCI iteratively generates synthetic instruction via object detection and trains the VG model with the generated data. We validate our framework in offline and online settings across diverse environments on different VG models. Experimental results show that accumulating synthetic data from GVCCI leads to a steady improvement in VG by up to 56.7% and improves resultant LGRM by up to 29.4%. Furthermore, the qualitative analysis shows that the unadapted VG model often fails to find correct objects due to a strong bias learned from the pre-training data. Finally, we introduce a novel VG dataset for LGRM, consisting of nearly 252k triplets of image-object-instruction from diverse manipulation environments. 
GVCCI: Lifelong Learning of Visual Grounding for Language-Guided Robotic Manipulation
 - _**标题**_: GVCCI:终身学习的视觉接地Language-Guided机械操纵
- _**摘要**_: Language-Guided机器人操纵(LGRM)是一个具有挑战性的任务,因为它需要一个机器人理解人类的指令来操作日常用品。最近LGRM方法依靠pre-trained视觉接地(VG)模型检测操纵对象,而不适应环境。这将导致性能下降由于大量域训练和实际数据之间的差距。一个简单的解决方案是收集额外的培训数据,但human-annotation的成本高得离谱。在本文中,我们提出了接地视野不断创建指令(GVCCI), LGRM终身学习框架,不断学习VG在没有人监督。GVCCI迭代生成合成指令通过对象检测和火车VG模型生成的数据。我们验证框架在线下和线上设置跨不同的环境不同的VG模型。实验结果表明,合成的数据积累GVCCI导致VG 56.7%稳步改善,提高合成LGRM 29.4%。此外,定性分析表明,不适应的VG模型往往无法找到正确的对象由于强烈的偏见从训练的数据。最后,我们引入一个新的LGRM VG的数据集,包括近252 k的三胞胎image-object-instruction来自不同操作环境。 
- _**url**_: http://arxiv.org/abs/2307.05963v1


## Paper 21 : Decomposing the Generalization Gap in Imitation Learning for Visual Robotic Manipulation
- _**Keywords**_: manipulation, robot
- _**Abstract**_: What makes generalization hard for imitation learning in visual robotic manipulation? This question is difficult to approach at face value, but the environment from the perspective of a robot can often be decomposed into enumerable factors of variation, such as the lighting conditions or the placement of the camera. Empirically, generalization to some of these factors have presented a greater obstacle than others, but existing work sheds little light on precisely how much each factor contributes to the generalization gap. Towards an answer to this question, we study imitation learning policies in simulation and on a real robot language-conditioned manipulation task to quantify the difficulty of generalization to different (sets of) factors. We also design a new simulated benchmark of 19 tasks with 11 factors of variation to facilitate more controlled evaluations of generalization. From our study, we determine an ordering of factors based on generalization difficulty, that is consistent across simulation and our real robot setup. 
Decomposing the Generalization Gap in Imitation Learning for Visual Robotic Manipulation
 - _**标题**_: 分解泛化差距模仿学习视觉机器人操作
- _**摘要**_: 使推广难模仿学习视觉机器人操纵?这个问题很难接近,但环境从一个机器人的角度通常可以分解成可列举的因素的变化,如光照条件或相机的位置。经验,归纳一些这些因素提出了比其他人更大的障碍,但现有研究并未精确多少每个因素导致泛化差距。对这个问题的答案,我们学习模仿学习政策模拟和实际机器人language-conditioned操纵任务量化的困难泛化不同因素(套)。我们也设计一个新的模拟基准19个任务有11个因素的变异促进泛化的更多控制的评估。从我们的研究中,我们确定一个排序基于泛化困难的因素,这在仿真和实际机器人设置是一致的。 
- _**url**_: http://arxiv.org/abs/2307.03659v1


## Paper 22 : BiRP: Learning Robot Generalized Bimanual Coordination using Relative Parameterization Method on Human Demonstration
- _**Keywords**_: manipulation, robot
- _**Abstract**_: Human bimanual manipulation can perform more complex tasks than a simple combination of two single arms, which is credited to the spatio-temporal coordination between the arms. However, the description of bimanual coordination is still an open topic in robotics. This makes it difficult to give an explainable coordination paradigm, let alone applied to robotics. In this work, we divide the main bimanual tasks in human daily activities into two types: leader-follower and synergistic coordination. Then we propose a relative parameterization method to learn these types of coordination from human demonstration. It represents coordination as Gaussian mixture models from bimanual demonstration to describe the change in the importance of coordination throughout the motions by probability. The learned coordinated representation can be generalized to new task parameters while ensuring spatio-temporal coordination. We demonstrate the method using synthetic motions and human demonstration data and deploy it to a humanoid robot to perform a generalized bimanual coordination motion. We believe that this easy-to-use bimanual learning from demonstration (LfD) method has the potential to be used as a data augmentation plugin for robot large manipulation model training. The corresponding codes are open-sourced in https://github.com/Skylark0924/Rofunc. 
BiRP: Learning Robot Generalized Bimanual Coordination using Relative Parameterization Method on Human Demonstration
 - _**标题**_: BiRP:学习机器人广义用双手的协调使用相对参数化方法对人类演示
- _**摘要**_: 人类用双手的操作可以执行更复杂的任务比简单的组合两个单武器,这是归功于之间的时空协调武器。然而,用双手的协调的描述机器人的仍然是一个开放的话题。这很难给出一个可辩解的协调模式,更不用说应用于机器人。在这项工作中,我们主要用双手的任务在人类的日常活动划分为两种类型:被领导,协同协调。然后,我们提出一个相对参数化方法从人类示范学习这些类型的协调。它代表协调从用双手的示范来描述高斯混合模型的变化的重要性,协调整个运动的概率。学会了协调表示可以推广到新任务参数同时确保时空协调。我们将演示使用方法合成运动和人类演示数据并将其部署到一个人形机器人执行广义用双手的协调运动。我们相信这个易于使用的用双手的学习示范(最晚完成日期)方法有潜力用作数据增加插件操纵机器人大型模型的训练。对应的代码是开源的https://github.com/Skylark0924/Rofunc。 
- _**url**_: http://arxiv.org/abs/2307.05933v1


## Paper 23 : Bi-Touch: Bimanual Tactile Manipulation with Sim-to-Real Deep Reinforcement Learning
- _**Keywords**_: manipulation, robot
- _**Abstract**_: Bimanual manipulation with tactile feedback will be key to human-level robot dexterity. However, this topic is less explored than single-arm settings, partly due to the availability of suitable hardware along with the complexity of designing effective controllers for tasks with relatively large state-action spaces. Here we introduce a dual-arm tactile robotic system (Bi-Touch) based on the Tactile Gym 2.0 setup that integrates two affordable industrial-level robot arms with low-cost high-resolution tactile sensors (TacTips). We present a suite of bimanual manipulation tasks tailored towards tactile feedback: bi-pushing, bi-reorienting and bi-gathering. To learn effective policies, we introduce appropriate reward functions for these tasks and propose a novel goal-update mechanism with deep reinforcement learning. We also apply these policies to real-world settings with a tactile sim-to-real approach. Our analysis highlights and addresses some challenges met during the sim-to-real application, e.g. the learned policy tended to squeeze an object in the bi-reorienting task due to the sim-to-real gap. Finally, we demonstrate the generalizability and robustness of this system by experimenting with different unseen objects with applied perturbations in the real world. Code and videos are available at https://sites.google.com/view/bi-touch/. 
Bi-Touch: Bimanual Tactile Manipulation with Sim-to-Real Deep Reinforcement Learning
 - _**标题**_: Bi-Touch:用双手的触摸与Sim-to-Real深的强化学习
- _**摘要**_: 用双手的操作与触觉反馈将人类机器人灵巧的关键。不如单臂探索这个主题设置,部分原因是合适的硬件的可用性以及有效的控制器设计的复杂性任务相对较大的政府行动空间。这里我们引入一个双臂触觉机器人系统(Bi-Touch)相结合的基于触觉体育馆2.0设置两个负担得起的工业水平与低成本机器人手臂高分辨率触觉传感器(TacTips)。我们提出一套专门针对触觉反馈的用双手的操作任务:bi-pushing, bi-reorienting bi-gathering。学习有效的政策,我们引入适当的奖励函数这些任务并提出一种新颖的goal-update机制与强化学习。我们也应用这些政策与触觉sim-to-real真实生活环境的方法。我们的分析突出和地址在sim-to-real应用遇到了一些挑战,例如了解政策倾向于紧缩对象在bi-reorienting任务由于sim-to-real差距。最后,我们将演示该系统的普遍性和健壮性通过尝试不同的看不见的对象应用扰动在现实世界中。代码和视频可在https://sites.google.com/view/bi-touch/上。 
- _**url**_: http://arxiv.org/abs/2307.06423v1


## Paper 24 : A Mixed Reality System for Interaction with Heterogeneous Robotic Systems
- _**Keywords**_: manipulation, robot
- _**Abstract**_: The growing spread of robots for service and industrial purposes calls for versatile, intuitive and portable interaction approaches. In particular, in industrial environments, operators should be able to interact with robots in a fast, effective, and possibly effortless manner. To this end, reality enhancement techniques have been used to achieve efficient management and simplify interactions, in particular in manufacturing and logistics processes. Building upon this, in this paper we propose a system based on mixed reality that allows a ubiquitous interface for heterogeneous robotic systems in dynamic scenarios, where users are involved in different tasks and need to interact with different robots. By means of mixed reality, users can interact with a robot through manipulation of its virtual replica, which is always colocated with the user and is extracted when interaction is needed. The system has been tested in a simulated intralogistics setting, where different robots are present and require sporadic intervention by human operators, who are involved in other tasks. In our setting we consider the presence of drones and AGVs with different levels of autonomy, calling for different user interventions. The proposed approach has been validated in virtual reality, considering quantitative and qualitative assessment of performance and user's feedback. 
A Mixed Reality System for Interaction with Heterogeneous Robotic Systems
 - _**标题**_: 混合现实系统与异构机器人系统的交互
- _**摘要**_: 日益蔓延的机器人服务和工业用途要求多才多艺,直观和便携式交互方法。特别是在工业环境中,运营商应该能够与机器人相处快,有效,并可能轻松的方式。为此,现实增强技术已经被用来实现高效管理和简化的相互作用,尤其是在制造业和物流流程。建筑在这,在本文中,我们提出一种基于混合现实的系统,它允许一个无处不在的接口异构机器人系统在动态场景中,用户参与不同的任务和需要与不同的机器人。通过混合现实,用户可以通过操纵与机器人交互的虚拟副本,这始终是与用户托管和提取交互是必要的。系统测试在模拟intralogistics设置,和不同的机器人需要零星的操作员干预,参与其他任务。在我们的设置考虑无人机和无人搬运车的存在不同程度的自主权,要求不同的用户干预。验证了该方法在虚拟现实,考虑的定量和定性评估性能和用户的反馈。 
- _**url**_: http://arxiv.org/abs/2307.05280v2


## Paper 25 : TRansPose: Large-Scale Multispectral Dataset for Transparent Object
- _**Abstract**_: Transparent objects are encountered frequently in our daily lives, yet recognizing them poses challenges for conventional vision sensors due to their unique material properties, not being well perceived from RGB or depth cameras. Overcoming this limitation, thermal infrared cameras have emerged as a solution, offering improved visibility and shape information for transparent objects. In this paper, we present TRansPose, the first large-scale multispectral dataset that combines stereo RGB-D, thermal infrared (TIR) images, and object poses to promote transparent object research. The dataset includes 99 transparent objects, encompassing 43 household items, 27 recyclable trashes, 29 chemical laboratory equivalents, and 12 non-transparent objects. It comprises a vast collection of 333,819 images and 4,000,056 annotations, providing instance-level segmentation masks, ground-truth poses, and completed depth information. The data was acquired using a FLIR A65 thermal infrared (TIR) camera, two Intel RealSense L515 RGB-D cameras, and a Franka Emika Panda robot manipulator. Spanning 87 sequences, TRansPose covers various challenging real-life scenarios, including objects filled with water, diverse lighting conditions, heavy clutter, non-transparent or translucent containers, objects in plastic bags, and multi-stacked objects. TRansPose dataset can be accessed from the following link: https://sites.google.com/view/transpose-dataset 
TRansPose: Large-Scale Multispectral Dataset for Transparent Object
 - _**标题**_: 为透明对象转置:大规模的多光谱数据集
- _**摘要**_: 透明的对象是我们日常生活中经常遇到,但承认他们带来了挑战传统视觉传感器由于其独特的材料特性,不认为从RGB或深度照相机。克服这个限制,热红外摄像机已经成为一个解决方案,提供改进的可见性和透明物体的形状信息。在本文中,我们目前的转置,第一次大规模多光谱数据集相结合的立体RGB-D,热红外图像(行动),和对象对促进透明的对象研究。数据集包括99透明物体,包括43家居用品,27日可回收垃圾,29化学实验室等价物,和12不透明的物体。它包含一个庞大的收集333819张图片和4000056注释,提供实例级分割的面具,真实的姿势,和完成深度信息。数据使用一个收购FLIR A65热红外相机(行动),两个英特尔RealSense L515 RGB-D相机,和一个Franka Emika熊猫机器人机械手。跨越87年序列,转置涵盖了各种具有挑战性的现实场景,包括对象装满水,不同的照明条件下,沉重的杂乱,不透明或半透明容器盛牛奶,对象在塑料袋里,和堆栈对象。置数据集可以从以下链接访问:https://sites.google.com/view/transpose-dataset 
- _**url**_: http://arxiv.org/abs/2307.05016v1


## Paper 26 : Robotic surface exploration with vision and tactile sensing for cracks detection and characterisation
- _**Abstract**_: This paper presents a novel algorithm for crack localisation and detection based on visual and tactile analysis via fibre-optics. A finger-shaped sensor based on fibre-optics is employed for the data acquisition to collect data for the analysis and the experiments. To detect the possible locations of cracks a camera is used to scan an environment while running an object detection algorithm. Once the crack is detected, a fully-connected graph is created from a skeletonised version of the crack. A minimum spanning tree is then employed for calculating the shortest path to explore the crack which is then used to develop the motion planner for the robotic manipulator. The motion planner divides the crack into multiple nodes which are then explored individually. Then, the manipulator starts the exploration and performs the tactile data classification to confirm if there is indeed a crack in that location or just a false positive from the vision algorithm. If a crack is detected, also the length, width, orientation and number of branches are calculated. This is repeated until all the nodes of the crack are explored.   In order to validate the complete algorithm, various experiments are performed: comparison of exploration of cracks through full scan and motion planning algorithm, implementation of frequency-based features for crack classification and geometry analysis using a combination of vision and tactile data. From the results of the experiments, it is shown that the proposed algorithm is able to detect cracks and improve the results obtained from vision to correctly classify cracks and their geometry with minimal cost thanks to the motion planning algorithm. 
Robotic surface exploration with vision and tactile sensing for cracks detection and characterisation
 - _**标题**_: 机器人与视觉和触觉传感表面探索裂缝检测和描述
- _**摘要**_: 本文提出一种新颖的算法基于视觉和触觉的裂纹本地化和检测分析,通过光纤。通行传感器基于光纤是用于收集数据的数据采集分析和实验。检测裂缝的可能位置相机是用来扫描一个环境运行时对象检测算法。一旦发现裂缝,创建一个全连通图从skeletonised版本的裂纹。最小生成树然后用于计算最短路径探索破解然后用于开发机器人机械手的运动计划。运动计划将裂缝分为多个节点,然后分别探讨了。然后,机械手开始探索和执行触觉数据分类确认如果确实有一个裂缝位置从视觉上还是假阳性的算法。如果检测到裂缝,长度,宽度,计算方向和数量的分支。重复此过程,直到所有的节点裂纹是探索。为了验证算法,完成各种实验执行:比较探索裂缝通过全扫描和运动规划算法,实现对裂缝的分类和基于的频率分布要素使用视觉和触觉的结合几何分析数据。从实验的结果,结果表明,该算法能够检测出裂缝,提高从视觉获得的结果正确分类以最小的成本由于裂缝及其几何运动规划算法。 
- _**url**_: http://arxiv.org/abs/2307.06784v1


## Paper 27 : Kinematically-Decoupled Impedance Control for Fast Object Visual Servoing and Grasping on Quadruped Manipulators
- _**Abstract**_: We propose a control pipeline for SAG (Searching, Approaching, and Grasping) of objects, based on a decoupled arm kinematic chain and impedance control, which integrates image-based visual servoing (IBVS). The kinematic decoupling allows for fast end-effector motions and recovery that leads to robust visual servoing. The whole approach and pipeline can be generalized for any mobile platform (wheeled or tracked vehicles), but is most suitable for dynamically moving quadruped manipulators thanks to their reactivity against disturbances. The compliance of the impedance controller makes the robot safer for interactions with humans and the environment. We demonstrate the performance and robustness of the proposed approach with various experiments on our 140 kg HyQReal quadruped robot equipped with a 7-DoF manipulator arm. The experiments consider dynamic locomotion, tracking under external disturbances, and fast motions of the target object. 
Kinematically-Decoupled Impedance Control for Fast Object Visual Servoing and Grasping on Quadruped Manipulators
 - _**标题**_: Kinematically-Decoupled阻抗控制快速对象视觉伺服和把握四足动物操纵者
- _**摘要**_: 我们提出一个控制管道凹陷(搜索、接近和把握)的对象,基于解耦的手臂运动链和阻抗控制,于一体的基于图像的视觉伺服(IBVS)。末端执行器运动解耦允许快速运动和恢复导致强劲的视觉伺服。整个方法和管道可以广义的移动平台(轮式或履带式车辆),但是最适合动态四足移动机械手由于对干扰的反应性。合规的阻抗控制器使机器人交互与人类和环境安全。我们演示该方法的性能和鲁棒性与各种实验在我们140公斤HyQReal四足机器人配备了7自由度机械臂。实验考虑动态运动,跟踪外部扰动下,快速运动的目标对象。 
- _**url**_: http://arxiv.org/abs/2307.04918v1


## Paper 28 : HA-ViD: A Human Assembly Video Dataset for Comprehensive Assembly Knowledge Understanding
- _**Abstract**_: Understanding comprehensive assembly knowledge from videos is critical for futuristic ultra-intelligent industry. To enable technological breakthrough, we present HA-ViD - the first human assembly video dataset that features representative industrial assembly scenarios, natural procedural knowledge acquisition process, and consistent human-robot shared annotations. Specifically, HA-ViD captures diverse collaboration patterns of real-world assembly, natural human behaviors and learning progression during assembly, and granulate action annotations to subject, action verb, manipulated object, target object, and tool. We provide 3222 multi-view, multi-modality videos (each video contains one assembly task), 1.5M frames, 96K temporal labels and 2M spatial labels. We benchmark four foundational video understanding tasks: action recognition, action segmentation, object detection and multi-object tracking. Importantly, we analyze their performance for comprehending knowledge in assembly progress, process efficiency, task collaboration, skill parameters and human intention. Details of HA-ViD is available at: https://iai-hrc.github.io/ha-vid. 
HA-ViD: A Human Assembly Video Dataset for Comprehensive Assembly Knowledge Understanding
 - _**标题**_: HA-ViD:人工组装视频数据集综合装配知识的理解
- _**摘要**_: 从视频理解综合装配知识对未来ultra-intelligent行业至关重要。实现技术突破,我们现在HA-ViD——人类第一次大会视频数据集特征代表工业装配场景,自然程序性知识获取过程,人与机器和一致的共享注释。特别是HA-ViD捕捉不同合作模式的实际装配,自然的人类行为和学习发展大会期间,表面变粗糙行动注释,动作动词,操纵对象,目标对象和工具。我们提供3222多视点、多模视频(每个视频包含一个装配任务),1.5帧,96 k时间标签和2 m空间标签。我们基准四个基本视频理解任务:动作识别,动作分割、目标检测和多目标跟踪。重要的是,我们分析其性能的理解知识在装配进度,流程效率、任务协作、技术参数和人类的意图。HA-ViD的细节可在:https://iai-hrc.github.io/ha-vid。 
- _**url**_: http://arxiv.org/abs/2307.05721v1


## Paper 29 : Forward Dynamics Estimation from Data-Driven Inverse Dynamics Learning
- _**Abstract**_: In this paper, we propose to estimate the forward dynamics equations of mechanical systems by learning a model of the inverse dynamics and estimating individual dynamics components from it. We revisit the classical formulation of rigid body dynamics in order to extrapolate the physical dynamical components, such as inertial and gravitational components, from an inverse dynamics model. After estimating the dynamical components, the forward dynamics can be computed in closed form as a function of the learned inverse dynamics. We tested the proposed method with several machine learning models based on Gaussian Process Regression and compared them with the standard approach of learning the forward dynamics directly. Results on two simulated robotic manipulators, a PANDA Franka Emika and a UR10, show the effectiveness of the proposed method in learning the forward dynamics, both in terms of accuracy as well as in opening the possibility of using more structured~models. 
Forward Dynamics Estimation from Data-Driven Inverse Dynamics Learning
 - _**标题**_: 从数据驱动的逆动力学学习前进动态估计
- _**摘要**_: 在本文中,我们提出估计远期学习模型的机械系统动力学方程的逆动力学和估算个体动力学组件。我们重新审视刚体动力学的经典配方,以推断的物理动力组件,如惯性和重力组件,从一个逆动力学模型。估计动态组件后,提出动态计算在封闭形式的函数学习的逆动力学。我们测试了该方法与几个机器学习模型基于高斯过程回归并把他们与标准方法的学习直接向前的动力。结果在两个模拟机器人机械手,熊猫Franka Emika UR10,显示了该方法的有效性在学习前进动力,无论是精度以及开放使用更加结构化的~模型的可能性。 
- _**url**_: http://arxiv.org/abs/2307.05093v1


## Paper 30 : Adaptive Compliant Robot Control with Failure Recovery for Object Press-Fitting
- _**Abstract**_: Loading of shipping containers for dairy products often includes a press-fit task, which involves manually stacking milk cartons in a container without using pallets or packaging. Automating this task with a mobile manipulator can reduce worker strain, and also enhance the efficiency and safety of the container loading process. This paper proposes an approach called Adaptive Compliant Control with Integrated Failure Recovery (ACCIFR), which enables a mobile manipulator to reliably perform the press-fit task. We base the approach on a demonstration learning-based compliant control framework, such that we integrate a monitoring and failure recovery mechanism for successful task execution. Concretely, we monitor the execution through distance and force feedback, detect collisions while the robot is performing the press-fit task, and use wrench measurements to classify the direction of collision; this information informs the subsequent recovery process. We evaluate the method on a miniature container setup, considering variations in the (i) starting position of the end effector, (ii) goal configuration, and (iii) object grasping position. The results demonstrate that the proposed approach outperforms the baseline demonstration-based learning framework regarding adaptability to environmental variations and the ability to recover from collision failures, making it a promising solution for practical press-fit applications. 
Adaptive Compliant Robot Control with Failure Recovery for Object Press-Fitting
 - _**标题**_: 自适应柔性机器人控制与故障恢复对象Press-Fitting
- _**摘要**_: 装载集装箱的乳制品通常包括一个压配合任务,其中包括手动堆垛牛奶盒没有使用托盘或集装箱包装。用移动机械手自动化这个任务可以减少工人的应变,以及提高集装箱装载过程的效率和安全性。本文提出了一种方法称为自适应的控制与集成故障恢复(ACCIFR),使移动机械手能够可靠地执行压配合任务。我们基地的方法演示上优于兼容的控制框架,这样我们整合成功的任务执行的监控和故障恢复机制。具体地说,我们通过距离和力反馈,监控执行检测碰撞在机器人执行压配合任务,并使用扳手测量分类碰撞的方向;这个信息通知随后的恢复过程。我们评估方法在微型容器设置,考虑变化(i)终端执行器的起始位置,(ii)目标配置,和(3)对象把握的位置。结果表明,该方法优于基线demonstration-based学习框架对于适应环境变化和碰撞故障恢复的能力,成为一个有前途的解决方案实际压配合应用。 
- _**url**_: http://arxiv.org/abs/2307.08274v1


## Paper 31 : A Versatile Door Opening System with Mobile Manipulator through Adaptive Position-Force Control and Reinforcement Learning
- _**Abstract**_: The ability of robots to navigate through doors is crucial for their effective operation in indoor environments. Consequently, extensive research has been conducted to develop robots capable of opening specific doors. However, the diverse combinations of door handles and opening directions necessitate a more versatile door opening system for robots to successfully operate in real-world environments. In this paper, we propose a mobile manipulator system that can autonomously open various doors without prior knowledge. By using convolutional neural networks, point cloud extraction techniques, and external force measurements during exploratory motion, we obtained information regarding handle types, poses, and door characteristics. Through two different approaches, adaptive position-force control and deep reinforcement learning, we successfully opened doors without precise trajectory or excessive external force. The adaptive position-force control method involves moving the end-effector in the direction of the door opening while responding compliantly to external forces, ensuring safety and manipulator workspace. Meanwhile, the deep reinforcement learning policy minimizes applied forces and eliminates unnecessary movements, enabling stable operation across doors with different poses and widths. The RL-based approach outperforms the adaptive position-force control method in terms of compensating for external forces, ensuring smooth motion, and achieving efficient speed. It reduces the maximum force required by 3.27 times and improves motion smoothness by 1.82 times. However, the non-learning-based adaptive position-force control method demonstrates more versatility in opening a wider range of doors, encompassing revolute doors with four distinct opening directions and varying widths. 
A Versatile Door Opening System with Mobile Manipulator through Adaptive Position-Force Control and Reinforcement Learning
 - _**标题**_: 多用途门开放系统与移动机械手通过自适应Position-Force控制和强化学习
- _**摘要**_: 机器人导航门的能力是至关重要的在室内环境中有效运行。因此,进行了大量的研究开发机器人能够打开特定的门。然而,不同组合的门把手,打开方向需要更多功能门打开机器人系统成功地在真实的环境中运作。在本文中,我们提出一个移动机械手系统,可以自动打开各种门没有先验知识。利用卷积神经网络,点云提取技术,在探索运动和外力测量,我们获得信息处理类型,姿势,门的特点。通过两种不同的方法,自适应position-force控制和强化学习,我们成功地打开门没有精确的轨迹或过度的外力。自适应position-force控制方法包括移动末端执行器在门开的方向而顺从地应对外部力量,确保安全性和机械手的工作空间。同时,深入强化学习策略最小化应用部队和消除不必要的动作,使稳定运行在门与不同的姿态和宽度。RL-based方法优于自适应position-force控制方法补偿外部力量,确保平稳运动,实现高效的速度。它减少了3.27倍所需的最大力量,改善运动平滑的1.82倍。然而,non-learning-based适应性position-force控制方法演示了在打开一个广泛的门更多才多艺,包括转动门有四个不同的开口方向和宽度不等。 
- _**url**_: http://arxiv.org/abs/2307.04422v1


